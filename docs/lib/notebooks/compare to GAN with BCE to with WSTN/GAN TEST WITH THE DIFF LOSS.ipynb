{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-04T20:12:23.769251Z",
     "start_time": "2024-08-04T20:12:23.764712Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 155
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T20:12:23.934669Z",
     "start_time": "2024-08-04T20:12:23.930662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 데이터 생성 (간단한 1차원 데이터)\n",
    "def generate_real_data(samples):\n",
    "    X1 = np.random.normal(0, 1, samples)\n",
    "    y = np.ones((samples, 1))\n",
    "    return X1, y"
   ],
   "id": "a3144defc5e554eb",
   "outputs": [],
   "execution_count": 156
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T20:12:24.106052Z",
     "start_time": "2024-08-04T20:12:24.102398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_fake_data(generator, samples, latent_dim):\n",
    "    noise = np.random.normal(0, 1, (samples, latent_dim))\n",
    "    X = generator.predict(noise)\n",
    "    y = np.zeros((samples, 1))\n",
    "    return X, y"
   ],
   "id": "66d3a7fa63ba5b71",
   "outputs": [],
   "execution_count": 157
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T20:12:24.248772Z",
     "start_time": "2024-08-04T20:12:24.244807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Binary Cross-Entropy loss function\n",
    "def bce_loss(y_true, y_pred):\n",
    "    return tf.keras.losses.binary_crossentropy(y_true, y_pred)"
   ],
   "id": "eda19bcdc199ef9a",
   "outputs": [],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T20:12:24.418161Z",
     "start_time": "2024-08-04T20:12:24.412665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generator model\n",
    "def build_generator(latent_dim):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(15, activation='relu', input_shape=(latent_dim,)))\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    return model"
   ],
   "id": "b85b46ba187d4b70",
   "outputs": [],
   "execution_count": 159
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T20:12:24.721113Z",
     "start_time": "2024-08-04T20:12:24.716600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Discriminator model\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(15, activation='relu', input_shape=(1,)))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model"
   ],
   "id": "e24920d60758829d",
   "outputs": [],
   "execution_count": 160
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T20:12:25.210583Z",
     "start_time": "2024-08-04T20:12:25.204591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training function for BCE loss\n",
    "def train_bce(generator, discriminator, combined, epochs, batch_size, latent_dim):\n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(batch_size):\n",
    "            # Train discriminator\n",
    "            X_real, y_real = generate_real_data(batch_size)\n",
    "            X_fake, y_fake = generate_fake_data(generator, batch_size, latent_dim)\n",
    "\n",
    "            d_loss_real = discriminator.train_on_batch(X_real, y_real)\n",
    "            d_loss_fake = discriminator.train_on_batch(X_fake, y_fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # Train generator\n",
    "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "            g_loss = combined.train_on_batch(noise, y_real)\n",
    "\n",
    "        print(f\"BCE Epoch {epoch + 1}/{epochs} [D loss: {d_loss:.6f}] [G loss: {g_loss:.6f}]\")\n"
   ],
   "id": "5a7e2d583b868606",
   "outputs": [],
   "execution_count": 161
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T20:12:25.963357Z",
     "start_time": "2024-08-04T20:12:25.959502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 주요 매개변수 설정\n",
    "latent_dim = 5\n",
    "epochs = 100\n",
    "batch_size = 32"
   ],
   "id": "521503603e3cb2b0",
   "outputs": [],
   "execution_count": 162
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T20:12:37.740523Z",
     "start_time": "2024-08-04T20:12:26.370674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generator와 Discriminator 모델 빌드\n",
    "generator_bce = build_generator(latent_dim)\n",
    "discriminator_bce = build_discriminator()\n",
    "discriminator_bce.compile(loss=bce_loss, optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002))\n",
    "\n",
    "# Combine 모델 빌드\n",
    "discriminator_bce.trainable = False\n",
    "z = tf.keras.Input(shape=(latent_dim,))\n",
    "fake_data = generator_bce(z)\n",
    "validity = discriminator_bce(fake_data)\n",
    "combined_bce = tf.keras.Model(z, validity)\n",
    "combined_bce.compile(loss=bce_loss, optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002))\n",
    "discriminator_bce.trainable = True\n",
    "\n",
    "# GAN 학습 (BCE Loss)\n",
    "train_bce(generator_bce, discriminator_bce, combined_bce, epochs, batch_size, latent_dim)"
   ],
   "id": "6c0626e89c89ea0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 38ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 18ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to list.__format__",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[163], line 16\u001B[0m\n\u001B[0;32m     13\u001B[0m discriminator_bce\u001B[38;5;241m.\u001B[39mtrainable \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# GAN 학습 (BCE Loss)\u001B[39;00m\n\u001B[1;32m---> 16\u001B[0m \u001B[43mtrain_bce\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgenerator_bce\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiscriminator_bce\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcombined_bce\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlatent_dim\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[161], line 17\u001B[0m, in \u001B[0;36mtrain_bce\u001B[1;34m(generator, discriminator, combined, epochs, batch_size, latent_dim)\u001B[0m\n\u001B[0;32m     14\u001B[0m     noise \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mnormal(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, (batch_size, latent_dim))\n\u001B[0;32m     15\u001B[0m     g_loss \u001B[38;5;241m=\u001B[39m combined\u001B[38;5;241m.\u001B[39mtrain_on_batch(noise, y_real)\n\u001B[1;32m---> 17\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBCE Epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m [D loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00md_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.6f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] [G loss: \u001B[39m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mg_loss\u001B[49m\u001B[38;5;132;43;01m:\u001B[39;49;00m\u001B[38;5;124;43m.6f\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: unsupported format string passed to list.__format__"
     ]
    }
   ],
   "execution_count": 163
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Wasserstein loss function\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_true * y_pred)"
   ],
   "id": "e1503958f3281f4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Generator model\n",
    "def build_generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(128, activation='relu', input_shape=(100,)))\n",
    "    model.add(layers.Dense(28*28, activation='tanh'))\n",
    "    model.add(layers.Reshape((28, 28, 1)))\n",
    "    return model"
   ],
   "id": "611221c9fa653cf2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Discriminator model\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=(28, 28, 1)))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    return model"
   ],
   "id": "8ecb1c0bc1f27205"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load and preprocess the MNIST dataset\n",
    "def load_data():\n",
    "    (train_images, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "    train_images = train_images / 127.5 - 1.0  # Normalize to [-1, 1]\n",
    "    train_images = np.expand_dims(train_images, axis=-1)\n",
    "    return train_images"
   ],
   "id": "b9372b8c5331d5df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Training function for Wasserstein loss\n",
    "def train_wasserstein(generator, discriminator, combined, epochs, batch_size, latent_dim, data):\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(data.shape[0] // batch_size):\n",
    "            # Train discriminator\n",
    "            real_data = data[np.random.randint(0, data.shape[0], batch_size)]\n",
    "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "            fake_data = generator.predict(noise)\n",
    "\n",
    "            d_loss_real = discriminator.train_on_batch(real_data, -np.ones((batch_size, 1)))\n",
    "            d_loss_fake = discriminator.train_on_batch(fake_data, np.ones((batch_size, 1)))\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # Train generator\n",
    "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "            g_loss = combined.train_on_batch(noise, -np.ones((batch_size, 1)))\n",
    "\n",
    "        d_losses.append(d_loss)\n",
    "        g_losses.append(g_loss)\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Wasserstein Epoch {epoch + 1}/{epochs}, Batch {batch + 1}/{data.shape[0] // batch_size} [D loss: {d_loss:.6f}] [G loss: {g_loss:.6f}]\")\n",
    "            save_images(generator, epoch + 1, latent_dim)\n",
    "\n",
    "    return d_losses, g_losses"
   ],
   "id": "10cc2da9a127c461"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function to save generated images\n",
    "def save_images(generator, epoch, latent_dim, examples=10, dim=(1, 10), figsize=(10, 1)):\n",
    "    noise = np.random.normal(0, 1, (examples, latent_dim))\n",
    "    generated_images = generator.predict(noise)\n",
    "    generated_images = generated_images.reshape(examples, 28, 28)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(examples):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        plt.imshow(generated_images[i], interpolation='nearest', cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'gan_generated_image_wasserstein_epoch_{epoch}.png')\n",
    "    plt.close()"
   ],
   "id": "44ee99a9045bbbe3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "latent_dim = 100\n",
    "epochs = 1000\n",
    "batch_size = 64"
   ],
   "id": "9795822aa00760ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data = load_data()",
   "id": "405484e67c677477"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Build and compile models for Wasserstein loss\n",
    "generator_wasserstein = build_generator()\n",
    "discriminator_wasserstein = build_discriminator()\n",
    "discriminator_wasserstein.compile(loss=wasserstein_loss, optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.00005))"
   ],
   "id": "b0d00f21192d6c73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Combine models\n",
    "discriminator_wasserstein.trainable = False\n",
    "z = tf.keras.Input(shape=(latent_dim,))\n",
    "fake_data = generator_wasserstein(z)\n",
    "validity = discriminator_wasserstein(fake_data)\n",
    "combined_wasserstein = tf.keras.Model(z, validity)\n",
    "combined_wasserstein.compile(loss=wasserstein_loss, optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.00005))\n",
    "discriminator_wasserstein.trainable = True"
   ],
   "id": "5bdb0e6605b5bd37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train the GAN with Wasserstein loss\n",
    "d_losses_wasserstein, g_losses_wasserstein = train_wasserstein(generator_wasserstein, discriminator_wasserstein, combined_wasserstein, epochs, batch_size, latent_dim, data)"
   ],
   "id": "75b1230c9dcdbc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d971ad0bd3ca4041"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
