<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[mogacko]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib\media\favicon.png</url><title>mogacko</title><link/></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Mon, 22 Jul 2024 10:29:15 GMT</lastBuildDate><atom:link href="lib\rss.xml" rel="self" type="application/rss+xml"/><pubDate>Mon, 22 Jul 2024 10:29:14 GMT</pubDate><ttl>60</ttl><dc:creator/><item><title><![CDATA[계획]]></title><description><![CDATA[ 
 <br><br>프로젝트의 목표에 맞는 데이터 특성에 가장 적합한 손실 함수를 찾기 위해 다양한 손실함수에 대해 공부한다. 각 로스함수의 수학적 정의 및 유도과정 이해, 각 로스함수가 사용되는 모델과 그 이유(어떤 모델과 문제에 적합한지), 장 단점 분석, 간단한 구현예제를 작성해보면서 이해해본다. 또한 다양한 학습방법 및 디퓨전모델에 대해서도 알아본다.<br>회의방법<br>
온라인(naver whale on) *zoom은 40분이상하려면 유료로 결제를 해야하기 때문에 whale on을 활용했습니다.<br>팀원 블로그<br>
박세준 <a rel="noopener" class="external-link" href="https://kepler-dev-3141.github.io/clip.html" target="_blank">https://kepler-dev-3141.github.io/clip.html</a><br>
신우석 <a rel="noopener" class="external-link" href="https://blog.naver.com/sws040201/223512772098" target="_blank">https://blog.naver.com/sws040201/223512772098</a><br>
김채연 <a rel="noopener" class="external-link" href="https://kcyeon0127.github.io/" target="_blank">https://kcyeon0127.github.io/</a><br><br><br><br>wassertein 손실함수의 정의 및 배경 특히 이를 이해하기 위해 필요한 개념인 Earth Mover's Distance (EMD) 에 대해 알아본다.<br><br><br>Wasserstein Distance를 최소화하는 방식으로 GAN을 훈련시키는 손실함수<br><br>GAN 에서 주로 사용하는 loss fuction인  bce loss의 문제점 때문에 나오게 되었습니다.<br>
BCE LOSS의 문제점<br>
*1. mode collapse(모드 붕괴): 생성자가 다양한 이미지를  만들어내지 못하고 비슷한 이미지만 계속해서 생성하는 경우<br>
2.  vanishing gradient(기울기 소실): bce 손실함수는 판별자가 점점 더 정확해짐에 따라 0 또는 1에 가까운 극단적인 값을 출력하게 만듭니다. 이는 판별자가 실제와 가짜 데이터를 잘 구분할수록 생성자가 받는 그래디언트 값이 매우 작아져서 학습이 정체되게  됩니다. 생성자는 더 이상 유의미한 피드백을 받지 못하고, 학습이 중지될 수 있습니다.<br><br> earth는 흙, 그래서 흙을 움직이는 거리라는 의미이다.<br>
*정의: 두 확률 분포 P와 Q 사이의 EMD는 P에서 Q로의 '흙'을 옮기는 최소 비용으로 정의된다. 즉, 생성된분포를 진짜와 동일하게 만드는 데 필요한 노력의 양을 추정하여 이 두 분포가 얼마나 다른지를 측정합니다. <br>분산은 같지만 평균이 다른 생성된 분포와 진짜 분포를 사용하며, 이들이 정규 분포라고 가정!<br><img alt="team-blog-코딩황제들-2024-week1.20240714212929.png" src="lib\media\team-blog-코딩황제들-2024-week1.20240714212929.png"><br>
함수는 생성된 분포를 이동해야 하는 거리와 양에 따라 다릅니다.(양과 거리의 함수)<br>
<img alt="lib/media/team-blog-코딩황제들-2024-week1.20240714225208.png" src="lib\media\team-blog-코딩황제들-2024-week1.20240714225208.png"><br>
<br>거리의 한계 없음:

<br>EMD는 두 분포 사이의 최적 운송 비용을 측정하며, 이 거리는 0과 1로 제한되지 않습니다. 즉, 분포 간의 차이가 클수록 거리가 계속 증가할 수 있습니다.


<br>기울기 유지:<br>
- EMD의 기울기는 분포 간 차이가 커질수록 계속해서 유의미한 값을 유지합니다. 이는 생성자가 받는 기울기가 0에 가까워지지 않음을 의미합니다.<br>
- 결과적으로, GAN의 생성자는 계속해서 학습할 수 있는 유의미한 피드백을 받게 되어 기울기 소실 문제를 겪지 않습니다.<br>
공식<br>
<img alt="lib/media/team-blog-코딩황제들-2024-week1.20240714234641.png" src="lib\media\team-blog-코딩황제들-2024-week1.20240714234641.png">
<br><br> inf(infimum): 하한, 주어진 범위에서의 최소값<br>
*γ(감마): P와 Q사이의 가능한 모든 운송 계획, P의 질량을 Q로 옮기는 방법을 의미합니다.<br>
E(기댓값): 주어진 결합 분포 γ에 대해, x와 y 사이의 거리에 대한 기대값을 의미합니다.<br><br><br>CLIP 학습 방법의 정의와 전반적인 개념 학습<br><br><br>
CLIP: Contrastive Language–Image Pre-training
<br>기존 SOTA<a data-href="1" href="1" class="internal-link" target="_self" rel="noopener">1</a>(<a rel="noopener" class="external-link" href="https://kepler-dev-3141.github.io/#fn-1-e3f99f632f1087fe" target="_blank">https://kepler-dev-3141.github.io/#fn-1-e3f99f632f1087fe</a>)&nbsp;Computer vision 시스템은 ImageNet과 같이 crowd-labeled 데이터셋을 이용하여 학습합니다. 이것은 정해진 카테고리 내에서 예측하도록 학습합니다. 이러한 학습 방법은 기존 데이터셋에 없는 라벨을 분류하기 위해서는 추가적인 데이터가 필요하고, 이것은 일반성과 사용성을 해치는 단점이 있습니다.<br>
자연어 처리 부분에서는 crowd-labeled 데이터셋을 사용하는 것 보다 웹 규모의 텍스트 모음을 사용하는 것이 좋은 결과를 가져온다는 선례가 있었습니다. 기존 SOTA Computer vision 시스템은 crowd-labeled 데이터셋을 사용하여 pre-training 하는 것이 일반적이었습니다. 이러한 방법을 Computer vision에도 적용하면 성능이 좋아지지 않을까 하며 만들어진 것이 CLIP입니다.<br><br><br>자연어를 supervision<a data-href="2" href="2" class="internal-link" target="_self" rel="noopener">2</a>(<a data-tooltip-position="top" aria-label="https://kepler-dev-3141.github.io/#fn-2-e3f99f632f1087fe)%EC%9C%BC%EB%A1%9C" rel="noopener" class="external-link" href="https://kepler-dev-3141.github.io/#fn-2-e3f99f632f1087fe)%EC%9C%BC%EB%A1%9C" target="_blank">https://kepler-dev-3141.github.io/#fn-2-e3f99f632f1087fe)으로</a> 사용하는 접근법입니다. 모델은 이미지와 자연어 문장을 한 쌍으로 학습하게 되며 이러한 접근법은 다음과 같은 장점을 가집니다.<br>
<br>crowd-sourced labeling에 비해서 자연어 지도를 확장하는 것이 쉽다.
<br>단순히 표현을 학습하는 것이 아니라 표현을 언어롸 연결시켜 zero-shot을 가능하도록 한다.
<br><br>기존 연구에서는 주로 MS-COCO, Visual Genome, YFCC100M 세 가지 데이터셋을 사용 해 왔습니다. MS-COCO와 Visual Genome는 crowd-sorced labeling 된 데이터셋이지만, 각각 약 10만 개의 적은 수의 학습용 이미지를 가지고 있는 문제가 있었습니다. YFCC100M은 1억 개의 이미지로 구성되어 있지만 이미지의 메타데이터가 부족하고 데이터의 품질이 들쭉날쭉하다는 문제가 있었습니다. 이러한 문제로 기존 데이터셋을 이용하지 않고 인터넷에서 4억개의 이미지와 텍스트 쌍을 수집하여 새로운 데이터셋을 구축하였습니다.<br><br>(이미지, 텍스트)를 한 쌍으로 하는 N개의 배치에 대하여 CLIP은 N × N개의 가능한 경우에 대하여 예측하도록 학습됩니다. N개의 올바른 경우에 대해서는 이미지와 텍스트 임베딩의 코사인 유사성을 최대화하고, N² - N개의 잘못된 임베딩에 대해서는 최소화합니다. 유사성 점수는 Symmetric Cross Entropy Loss를 이용하여 최적화합니다. 학습 중 사용된 유일한 데이터 증강은 크기가 조정된 정사각형 자르기입니다.<br><img alt="Pasted image 20240715002034.png" src="https://kepler-dev-3141.github.io/pasted-image-20240715002034.png" referrerpolicy="no-referrer"><br><br>이미지 인코더는 변형된<br>
Gloabal Average Pooling을 Attention Pooling으로 대체한 ResNet-50과 임베딩에 Layer Normalization을 적용한 Vision Transformer 두 아키텍처를 고려하였습니다.<br>텍스트 인코더는 트랜스포머를 사용했습니다. 기본 크기로 6300만 매개변수의 12 레이어, 512폭의 모델을 8개의 attention head와 함께 사용했습니다. 트랜스포머는 49,152 크기의 소문자 바이트 페어 인코딩으로 텍스트를 처리합니다.<br><br>
<br>유연하고 일반적<br>
CLIP 모델은 zero-shot 방식으로 사용하도록 의도되었고 실제로도 작업에서 zero-shot으로 수행을 할 수 있다. 특히 ImageNet모델에서는 발생하지 않았던 OCR 학습이 일어났다.
<br>자연어 이해<br>
CLIP은 이미지와 자연어를 쌍으로 학습하여 텍스트 기반 이미지 검색 또는 이미지 분류를 할 수 있습니다.
<br>구체적이나 복잡한 작업에서는 성능 저하<br>
CLIP 모델은 일반적인 객체를 인식하는 부분에서는 좋은 성능을 보여주지만, 이미지에서 차량이 얼마나 가까운지, 자동차 또는 꽃 종류를 구분하는 작업에서는 성능이 좋지 못함을 보여주었다.
<br>데이터 편향성<br>
CLIP은 데이터를 인터넷에서 수집하여 학습하므로 사회적 편향을 학습하게 되는 문제가 있다.
<br>이 글은 아래의 글들을 참고하여 작성되었습니다.<br><a rel="noopener" class="external-link" href="https://arxiv.org/abs/2103.00020" target="_blank">https://arxiv.org/abs/2103.00020</a><br><a rel="noopener" class="external-link" href="https://openai.com/index/clip/" target="_blank">https://openai.com/index/clip/</a><br><a data-tooltip-position="top" aria-label="https://velog.io/@conel77/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-CLIP-Learning-Transferable-Visual-Models-From-Natural-Language-Supervision%EC%9E%91%EC%84%B1%EC%A4%91" rel="noopener" class="external-link" href="https://velog.io/@conel77/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-CLIP-Learning-Transferable-Visual-Models-From-Natural-Language-Supervision%EC%9E%91%EC%84%B1%EC%A4%91" target="_blank">https://velog.io/@conel77/논문-리뷰-CLIP-Learning-Transferable-Visual-Models-From-Natural-Language-Supervision작성중</a><br><a rel="noopener" class="external-link" href="https://ffighting.net/deep-learning-paper-review/multimodal-model/clip/#3_CLIP" target="_blank">https://ffighting.net/deep-learning-paper-review/multimodal-model/clip/#3_CLIP</a><br><br><br>categorical cross entropy (CCE)에 대해 알아본다.<br><br>Categorical Cross Entropy : 분류해야할 클래스가 3개 이상인 멀티 클래스 분포에 사용하며 one-hot 형태이다.<br>ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ<br>정보량(놀람) : 확률과 반비례. 확률이 1이면 (0~1) 정보량은 0<br>−log(pi​),&nbsp;pi​&nbsp;=사건의&nbsp;확률​<br>​<br>cross entropy는 두 가지 확률 분포간의 차이를 나타낼 수 있다.<br>H(P,&nbsp;Q)=−n∑ipi​log(qi​)​<br>p는 실제 확률분포, q는 예측한 확률분포.<br>​<br>​Categorical Cross Entropy는 분류해야할 클래스가 3개 이상인 멀티 클래스 분포에 사용하며,<br>라벨이 one-hot 형태일 때 사용된다. ex) [1,0,0], [0,1,0], [0,0,1]<br>( 라벨이 정수 형태인 경우 Sparse_Catecorical Cross Entropy 사용. ex) [0, 1, 2, ... ] )<br>L=−1N​N∑j=1C∑i=1pij​log(qij​)​<br>N은 훈련 샘플의 갯수, C는 클래수의 갯수<br>ont-hot 형태이기 때문에 정답에 가까워 질수록 0에 수렴한다.<br> 확률이 1이면 정보량은 0이기 때문에 때문<br>softmax 활성함수와 같이 쓰이는 경우가 많아 softmax activation function이라고도 불린다.<br>softmax : 로짓(점수)이 있는 벡터를 받아 합이 1인 확률을 가진 벡터를 출력<br>[출처] <a data-tooltip-position="top" aria-label="https://blog.naver.com/sws040201/223512772098" rel="noopener" class="external-link" href="https://blog.naver.com/sws040201/223512772098" target="_blank">Categorical Cross Entropy</a>|작성자 <a data-tooltip-position="top" aria-label="https://blog.naver.com/sws040201" rel="noopener" class="external-link" href="https://blog.naver.com/sws040201" target="_blank">sws040201</a><br><br><br>Binary Cross-Entropy (BCE) Loss에 대한 개념을 학습한다.<br><br>Binary Cross-Entropy (BCE) Loss란?<br>
이진 분류 문제에서 실제 클래스와 예측 확률 사이의 차이를 측정하는 손실 함수<br><br><img alt="lib/media/team-blog-코딩황제들-2024-week1.모각코 1일차 회의 인증.png" src="lib\media\team-blog-코딩황제들-2024-week1.모각코-1일차-회의-인증.png"><img alt="lib/media/team-blog-코딩황제들-2024-week1.모각코 1일차 시간 인증.jpg" src="lib\media\team-blog-코딩황제들-2024-week1.모각코-1일차-시간-인증.jpg">]]></description><link>team's-blog\코딩황제들\7월-14일-모각코.html</link><guid isPermaLink="false">team's blog/코딩황제들/7월 14일 모각코.md</guid><pubDate>Mon, 22 Jul 2024 10:27:52 GMT</pubDate><enclosure url="lib\media\team-blog-코딩황제들-2024-week1.20240714212929.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\team-blog-코딩황제들-2024-week1.20240714212929.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[계획]]></title><description><![CDATA[ 
 <br><br>프로젝트의 목표에 맞는 데이터 특성에 가장 적합한 손실 함수를 찾기 위해 다양한 손실함수에 대해 공부한다. 각 로스함수의 수학적 정의 및 유도과정 이해, 각 로스함수가 사용되는 모델과 그 이유(어떤 모델과 문제에 적합한지), 장 단점 분석, 간단한 구현예제를 작성해보면서 이해해본다. 또한 다양한 학습방법 및 디퓨전모델에 대해서도 알아본다.<br>회의방법<br>
온라인(naver whale on) *zoom은 40분이상하려면 유료로 결제를 해야하기 때문에 whale on을 활용했습니다.<br>팀원 블로그<br>
박세준 <a rel="noopener" class="external-link" href="https://kepler-dev-3141.github.io/clip.html" target="_blank">https://kepler-dev-3141.github.io/clip.html</a><br>
신우석 <a rel="noopener" class="external-link" href="https://blog.naver.com/sws040201/223512772098" target="_blank">https://blog.naver.com/sws040201/223512772098</a><br>
김채연 <a rel="noopener" class="external-link" href="https://kcyeon0127.github.io/" target="_blank">https://kcyeon0127.github.io/</a><br><br><br><br> Earth Mover's Distance (EMD) 개념을 바탕으로 wassertein distance 및 wassertein 손실함수에 대한 개념을 확실히 정립한다.<br><br><br>러시아 수학자 Leanid Vaserstein 의 이름을 딴 것으로 Roland Dobtushin 교수가 1970년에 확률론에 도입한 것이다.<br>
<img alt="Pasted image 20240721221302.png" src="lib\media\pasted-image-20240721221302.png"><br>
GAN에서 discriminator가 학습도중에 잘 죽는 현상이 나타나는데, 이를 해결하고자 하는것이 wasserstain loss이다. 이는 wasserstein distance를 최소화시키는 것이 목표이다.<br>
위식에서 위의 내용이 Wasserstein distance의 정의다.<br>여기에서 <img src="https://t1.daumcdn.net/cfile/tistory/9984A03359A4215B2E" referrerpolicy="no-referrer"> 는 두 확률분포 , P,Q의 결합확률분포(joint distribution)들을 모은 집합이고, 그 중에&nbsp;<br><img src="https://t1.daumcdn.net/cfile/tistory/991E073359A4217412" referrerpolicy="no-referrer">는 그 중 하나입니다. 즉 모든 결합확률분포&nbsp;<img src="https://t1.daumcdn.net/cfile/tistory/9951703359A4218E22" referrerpolicy="no-referrer">&nbsp;중에서 d(x,y)의 기댓값을 가장 작게 추정한 값을 의미합니다. <br>즉 두 확률분포의 연관성을 측정하여 그 거리의 기대값이 가장 작을때의 distance를 wasserstein distance라고 얘기를 합니다. <br><br>주변 확률 분포 P와 Q:<br>
<br>
P와 Q는 각각 X와 Y의 주변 확률 분포입니다.

<br>
주변 확률 분포는 각각의 확률 변수가 따르는 분포의 모양을 나타냅니다.

<br>
예를 들어, P와 Q가 모두 정규 분포라면, X와 Y는 각각 정규 분포를 따릅니다.<br>
결합 확률 분포 γ:

<br>
γ는 X와 Y의 결합 확률 분포로, w에 따라 샘플링된 X와 Y의 쌍의 분포를 나타냅니다.

<br>
결합 확률 분포는 두 확률 변수 간의 관계(의존성)를 나타냅니다.

<br>
다양한 γ는 서로 다른 X와 Y의 결합 방식을 의미합니다.

<br><br><img alt="Pasted image 20240721231544.png" src="lib\media\pasted-image-20240721231544.png"><br>
표본 공간에서 w를 하나 샘플링 하면 X(w)와 Y(w)를 뽑을 수 있고 이때 두 점 간의 거리 d(X(w),Y(w)) 역시 계산 할수 있다.<br><br><img alt="Pasted image 20240721225330.png" src="lib\media\pasted-image-20240721225330.png"><br>
샘플링을 계속 할수록 (X, Y)의 결합 확률 분포 γ의 윤곽이 나오게 더불어서 (P, Q)는 γ의 주변확률분포가 됩니다.<br><br><img alt="Pasted image 20240721225402.png" src="lib\media\pasted-image-20240721225402.png"><br>
이때 γ가 두 확률변수 X, Y의 연관성을 어떻게 측정하느냐에 따라 d(X, Y)의 분포가 달라지게 됩니다.<br><br><img alt="Pasted image 20240721225433.png" src="lib\media\pasted-image-20240721225433.png"><br>
주의할 점은 P와 Q는 바뀌지 않기 때문에 각 X와 Y 가 분포하는 모양은 변하지 않습니다. 다만 w에 따라 뽑히는 경향이 달라질 뿐이다.<br><br>
<br>
P와 Q:

<br>X는 정규 분포 P를 따릅니다.
<br>Y는 균등 분포 Q를 따릅니다.


<br>
결합 확률 분포 γ1​:

<br>γ1​에서는 X가 클 때 Y도 클 확률이 높습니다.
<br>즉, ω가 큰 값을 가질 때 X(ω)와 Y(ω) 모두 큰 값을 가질 경향이 있습니다.


<br>
결합 확률 분포 γ2​:
- γ2​에서는 X가 클 때 Y가 작을 확률이 높습니다.
- 즉, ω가 큰 값을 가질 때 X(ω)는 큰 값을, Y(ω)는 작은 값을 가질 경향이 있습니다.
Copy
요약

<br>
변하지 않는 것:

<br>P와 Q의 모양(즉, X와 Y의 분포)은 변하지 않습니다.
<br>X는 항상 정규 분포를 따르고, Y는 항상 균등 분포를 따릅니다.


<br>
변하는 것:

<br>γ에 따라 X와 Y가 뽑히는 경향이 달라집니다.
<br>즉, 두 변수 X와 Y의 결합 관계(의존성)가 달라집니다.


<br><br><img alt="Pasted image 20240721225502.png" src="lib\media\pasted-image-20240721225502.png"><br>
Wasserstein distance 는 이렇게 여러가지 γ중에서 d(X, Y) 의 기댓값이 가장 작게 나오는 확률분포를 취하게 된다.<br>그래서 Wasserstein GAN은 이 Wasserstein distance를 이용해서 GAN의 문제를 푸는 것이다.<br>출처: <a rel="noopener" class="external-link" href="https://dogfoottech.tistory.com/185" target="_blank">https://dogfoottech.tistory.com/185</a> [Magritte 기술블로그:티스토리]]]></description><link>team's-blog\코딩황제들\7월-21일-모각코.html</link><guid isPermaLink="false">team's blog/코딩황제들/7월 21일 모각코.md</guid><pubDate>Sun, 21 Jul 2024 14:36:57 GMT</pubDate><enclosure url="lib\media\pasted-image-20240721221302.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\pasted-image-20240721221302.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[코딩황제들]]></title><description><![CDATA[ 
 ]]></description><link>team's-blog\코딩황제들\코딩황제들.html</link><guid isPermaLink="false">team's blog/코딩황제들/코딩황제들.md</guid><pubDate>Mon, 15 Jul 2024 18:19:59 GMT</pubDate></item><item><title><![CDATA[team's blog]]></title><description><![CDATA[ 
 ]]></description><link>team's-blog\team's-blog.html</link><guid isPermaLink="false">team's blog/team's blog.md</guid><pubDate>Mon, 15 Jul 2024 18:16:01 GMT</pubDate></item><item><title><![CDATA[팀 소개]]></title><description><![CDATA[ 
 <br><br>안녕하세요 저희는 코딩황제들입니다. 저희 팀은 인공지능학과 4명의 학생으로 구성되어 있습니다.<br><br>박세준 <a rel="noopener" class="external-link" href="https://kepler-dev-3141.github.io/clip.html" target="_blank">https://kepler-dev-3141.github.io/clip.html</a><br>
신우석 <a rel="noopener" class="external-link" href="https://blog.naver.com/sws040201/223512772098" target="_blank">https://blog.naver.com/sws040201/223512772098</a><br>
김채연<br><br>저희의 목표는 생성형 AI를 만들기 위한 모델 디자인 및 구조를 짜기 위해 이론적인 공부를 심도 있게 진행하는 것입니다.<br><br>매주  22:00 ~ 01:00<br>
1회차 : 2024/07/14 22:00 ~ 01:00<br>
2회차 : 2024/07/21&nbsp;22:00 ~ 01:00<br>
3회차 : 2024/07/28&nbsp;22:00 ~ 01:00<br>
4회차 : 2024/08/04&nbsp;22:00 ~ 01:00<br>
5회차 : 2024/08/11&nbsp;22:00 ~ 01:00<br>
6회차 : 2024/08/18&nbsp;22:00 ~ 01:00<br>]]></description><link>team's-blog\코딩황제들\index.html</link><guid isPermaLink="false">team's blog/코딩황제들/index.md</guid><pubDate>Sun, 14 Jul 2024 16:17:49 GMT</pubDate></item><item><title><![CDATA[index]]></title><description><![CDATA[ 
 ]]></description><link>mogacko\team's-blog\코딩황제들\index.html</link><guid isPermaLink="false">mogacko/team's-blog/코딩황제들/index.md</guid><pubDate>Sun, 14 Jul 2024 14:30:27 GMT</pubDate></item><item><title><![CDATA[7월 28일 모각코]]></title><description><![CDATA[ 
 ]]></description><link>team's-blog\코딩황제들\7월-28일-모각코.html</link><guid isPermaLink="false">team's blog/코딩황제들/7월 28일 모각코.md</guid><pubDate>Wed, 10 Jul 2024 21:24:39 GMT</pubDate></item><item><title><![CDATA[8월 4일 모각코]]></title><description><![CDATA[ 
 ]]></description><link>team's-blog\코딩황제들\8월-4일-모각코.html</link><guid isPermaLink="false">team's blog/코딩황제들/8월 4일 모각코.md</guid><pubDate>Wed, 10 Jul 2024 21:24:39 GMT</pubDate></item><item><title><![CDATA[8월 11일 모각코]]></title><description><![CDATA[ 
 ]]></description><link>team's-blog\코딩황제들\8월-11일-모각코.html</link><guid isPermaLink="false">team's blog/코딩황제들/8월 11일 모각코.md</guid><pubDate>Wed, 10 Jul 2024 21:24:40 GMT</pubDate></item><item><title><![CDATA[8월 18일 모각코]]></title><description><![CDATA[ 
 ]]></description><link>team's-blog\코딩황제들\8월-18일-모각코.html</link><guid isPermaLink="false">team's blog/코딩황제들/8월 18일 모각코.md</guid><pubDate>Wed, 10 Jul 2024 21:24:40 GMT</pubDate></item></channel></rss>