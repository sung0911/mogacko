# 계획
energy-based model, 특히 energy-based GAN에 대해 알아보기 및 대조학습코드 구현 

**회의방법**
온라인(naver whale on) *zoom은 40분이상하려면 유료로 결제를 해야하기 때문에 whale on을 활용했습니다. 

**팀원 블로그**
박세준 https://kepler-dev-3141.github.io/
신우석 https://blog.naver.com/sws040201/
김채연 https://kcyeon0127.github.io/


# 결과

## 준혁: 
### 목표: 
Energy-Based Model에 대한 기본개념과 볼츠만 분포에대해 알아본다. 또한 볼츠만 분포에서 온도가 시스템의 상태에 미치는 영향을 알아본다.

### 결과:
##### 1. EBM이란?:
*머신러닝 모델 중 하나로, 주어진 입력 데이터에 대해 "에너지"라는 값을 할당합니다. 이 에너지는 어떤 상태나 결과가 얼마나 "좋은지" 또는 "가능한지"를 나타냅니다. EBM은 에너지가 낮은 상태를 찾아내는 것이 목표입니다.*
- **낮은 에너지:** 좋은 상태나 가능성이 높은 결과.
- **높은 에너지:** 나쁜 상태나 가능성이 낮은 결과.
*EX)
 예를 들어, 이미지 속의 고양이 모습을 찾는다면, 고양이 이미지에 낮은 에너지를 할당하고, 다른 동물이나 배경에 높은 에너지를 할당하는 것입니다.*

##### 2. 볼츠만 분포 (Boltzmann Distribution):
볼츠만 분포는 EBM에서 매우 중요한 역할을 하는 개념입니다.
이 개념을 머신러닝에 적용해 보면, 볼츠만 분포는 에너지가 낮을수록 그 상태가 나올 확률이 높다는 것을 의미합니다.

###### 2-1. 볼츠만 분포 수식

$P(x) = \frac{e^{-E(x)/T}}{Z}$

- $P(x)$: 상태 $x$가 발생할 확률
- $E(x)$: 상태 $x$의 에너지
- $T$: 온도
- $Z$: 정규화 상수 (모든 상태의 확률 합이 1이 되도록 조정하는 역할)

*이 수식은 온도가 높을수록 더 다양한 상태가 가능하지만, 온도가 낮아질수록 에너지가 낮은 상태만 가능해진다는 것을 보여줍니다.*

###### **2-2. 볼츠만 분포에서 온도의 의미**

***온도가 높을 때**: 시스템은 다양한 상태(높은 에너지, 낮은 에너지 모두)를 가질 수 있습니다. 즉, 다양한 상태들이 나타날 가능성이 있습니다. 왜냐하면 온도가 높을수록 분자들이 더 활발하게 움직입니다. 이것은 시스템이 높은 에너지를 가진 상태를 포함한 다양한 상태를 쉽게 탐색할 수 있다는 의미입니다.*

*장점: 모델이 다양한 상태(높은에너지와 낮은 에너지를 모두 포함)를 살펴보면서 더나은 전역 최솟값을 찾을 가능성을 높힌다.

*단점: 안정적인 상태에 수렴하지 못하고 불안정한 상태를 유지할 수있다.*


***온도가 낮을때**:  시스템은 주로 에너지가 낮은 상태에 머무르게 됩니다. 즉, 안정적인 상태만을 나타내는 경향이 강해집니다. 왜냐하면 분자들이 덜 활발하게 움직이기 때문에 이것은 시스템이 에너지가 낮은 상태에 머무르는 경향이 강해진다는 의미입니다.*

*장점: 시스템에너지가 낮은 즉 안정적인 상태를 더 잘찾아내게 된다.

*단점: 너무 낮은 온도에서는 시스템이 현재상태에 갇힐수 있어서 더 나은 상태를 탐색하지 못할수가 있다.*

***결론적으로 적절한 온도 조절이 필요하며, 보통 탐색 초기에는 높은 온도로 시작하고, 점차 온도를 낮춰가며 에너지가 낮은 최적 상태를 찾아가는 방식이 효과적입니다.***


## 세준:
### 목표:
EBGAN을 MNIST Dataset에 대하여 구현해보며 이해하기

### 결과:
MNIST Dataset에 대하여 GAN이 구현된 코드를 이용하여 EBGAN을 구현해 보았다. 원본 코드가 있는 블로그의 URL을 기재하려고 했으나 블로그가 닫혀 기재하지 못했다. Discriminator model을 Auto Encoder으로 수정하였고 network가 업데이트 되는 부분을 수정하였다. 하지만 숫자 이미지를 잘 생성하지 못하고 노이즈 데이터를 생성하였다.

#### Code

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
from torch.autograd import Variable as Variable
from torch.utils.data import DataLoader, Dataset
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import numpy as np
import time
import random
  
if torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device("cpu")
```

```python
MNIST_transform = transforms.Compose([transforms.Resize((28,28)),
                                      transforms.ToTensor(),
                                      transforms.Normalize((0.5,), (0.5,))])
  
trainset = datasets.MNIST(root='./data/MNIST/',
                           train=True,
                           download = True,
                           transform=MNIST_transform)
  
testset = datasets.MNIST(root='./data/MNIST/',
                           train=False,
                           download = True,
                           transform=MNIST_transform)
```

```python
train_loader = DataLoader(trainset, batch_size = 32, shuffle = True)
test_loader = DataLoader(testset, batch_size = 32, shuffle = True)
  
dataloaders = [train_loader, test_loader]
```

```python
image, label = next(iter(train_loader))
  
plt.figure(figsize=(16, 16))
for i in range(4):
    plt.subplot(4, 4, i+1)
    plt.imshow(image[i].squeeze(0), cmap='gray_r')
    plt.title(label[i].numpy())
  
image, label = next(iter(test_loader))

plt.figure(figsize=(16, 16))
for i in range(4):
    plt.subplot(4, 4, i+1)
    plt.imshow(image[i].squeeze(0), cmap='gray_r')
    plt.title(label[i].numpy())
```

```python
class gen(nn.Module):
  def __init__(self):
    super(gen, self).__init__()
    self.main = nn.Sequential(
        nn.Linear(100, 256),
        nn.ReLU(),
        nn.Linear(256, 512),
        nn.ReLU(),
        nn.Linear(512, 1024),
        nn.ReLU(),
        nn.Linear(1024, 28 * 28),
        nn.Tanh(),
    )
  
  def forward(self, x):
    output = self.main(x)
    output = output.view(x.size(0), 1, 28, 28)
    return output
```

```python
class autoencoder(nn.Module):
    def __init__(self):
        super(autoencoder, self).__init__()
  
        self.encoder = nn.Sequential(
            nn.Linear(28 * 28, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 4),
        )
        self.decoder = nn.Sequential(
            nn.Linear(4, 64),
            nn.ReLU(),
            nn.Linear(64, 128),
            nn.ReLU(),
            nn.Linear(128, 28 * 28),
            nn.Sigmoid(),
        )
  
    def forward(self, x):
        x = x.view(x.size(0), 28 * 28)
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return nn.MSELoss()(decoded, x)
```

```python
criterion = nn.MSELoss()
learning_rate = 0.001
num_epochs = 50
batch_size = 32
m = 10
  
G = gen().to(device)
D = autoencoder().to(device)
  
optimizer_gen = torch.optim.Adam(G.parameters(), lr=learning_rate)
optimizer_dis = torch.optim.Adam(D.parameters(), lr=learning_rate)
```

```python
from google.colab import drive
drive.mount('/content/drive')
```

```python
random_plot_data = torch.randn((batch_size, 100)).to(device)
  
start_time = time.time()
for epoch in range(num_epochs):
  print('-----------------------------------')
  print('{} Epoch'.format(epoch))
  for n, (x, target) in enumerate(train_loader):
    x, label = x.to(device), torch.ones((batch_size, 1)).to(device)
  
     D.zero_grad()
     z = torch.randn((batch_size, 100)).to(device)
     gen_data = G(z).detach()
     real_eng = criterion(D(x), x)
     fake_eng = criterion(D(gen_data), gen_data)
     D_loss = real_eng + torch.clamp(m - fake_eng, min=0.0)
     D_loss.backward()
     optimizer_dis.step()
  
     G.zero_grad()
     z = torch.randn((batch_size, 100)).to(device)
     gen_data = G(z)
     G_loss = criterion(D(gen_data), gen_data)
     G_loss.backward()
     optimizer_gen.step()
  
    if n == batch_size - 1:
        print(f"Dis Loss: {D_loss}")
        print(f"Gen Loss: {G_loss}\n")
        gen_data_plot = G(random_plot_data).cpu().detach()
        fig = plt.figure(figsize = (10,7))
        fig.suptitle(f"Generated Images After {epoch} Epochs", fontsize=16)
        for i in range(16):
            ax = plt.subplot(4, 4, i + 1)
            plt.imshow(gen_data_plot[i].reshape(28, 28), cmap="gray_r")
            plt.xticks([])
            plt.yticks([])
        plt.show()
  
elapsed_time = time.time() - start_time
print('\nElapsed time : {}s'.format(elapsed_time))

  
torch.save(G.state_dict(), '/content/drive/MyDrive/BasicML/EBGAN_G_mnist_model.pt')
torch.save(D.state_dict(), '/content/drive/MyDrive/BasicML/EBGAN_D_mnist_model.pt')
```

#### Result Image

아래는 50번째 Epoch의 image 생성 결과이다.

![Pasted image 20240819221325.png](https://kepler-dev-3141.github.io/pasted-image-20240819221325.png)





## 우석:

### 목표 :

energy based model에 대해 알아본다.

### 결과 :

ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ

energy-based model (EBM)은 스칼라 값 함수인 에너지 함수이다.

F(x,y)에서 x는 관측 변수이고, y는 예측해야 할 변수이다.

여기서 y를 생성하는게 아니고 x와 y를 주면 이 둘이 서로 호환되는지 아닌지를 알려준다.

y가 x와 호환될수록 F(x,y)는 낮은 값을, 호환되지 않을수록 F(x,y)는 높은 값을 가진다.

데이터에서 나오는 좋은 샘플에 낮은 에너지를 주고, 그 외엔 높은 에너지를 준다.

추론 과정은 어떤 지점에서 시작하여 가까운 에너지가 없는 지점을 찾아가는 것이다.

​

학습은 대조방법과 정규화/구조화 방법 두 가지가 존재한다.

대조방법은 모델이 학습 과정에서 데이터 샘플을 보고,

이러한 샘플의 에너지를 낮추기 위해(모델이 이 데이터를 더 잘 맞추도록) 에너지 값을 줄이는 방향으로 매개변수를 조정한다.

모델이 데이터 샘플의 에너지를 낮추기 위해, 경사하강법을 사용해 에너지 함수의 매개변수를 업데이트한다.

경사하강법은 에너지 함수의 기울기(gradient)를 계산해, 매개변수가 그 기울기 방향으로 이동하면서 에너지를 최소화하도록 한다.

데이터 매니폴드는 실제 데이터가 존재하는 공간을 나타낸다.

대조방법에서는 매니폴드의 "측면"에 있는 에너지가 더 높아야 한다고 말하는데, 이는 매니폴드(실제 데이터)에 가까운 데이터는 낮은 에너지를,

매니폴드 바깥에 있는 데이터(비현실적인 데이터)는 높은 에너지를 가지도록 학습해야 한다는 의미이다

대조학습의 성능은 어떤 샘플이 부정적인 샘플(에너지를 높여야 하는 샘플)인지 찾는 방법에 크게 좌우된다.

부정적인 대조샘플을 찾는 과정이 고차원 공간에서는 매우 어려워지고, 이 때문에 대조방법이 비효율적이 될 수 있다.

**[출처]** [energy based model (8/08)](https://blog.naver.com/sws040201/223541476039)|**작성자** [sws040201](https://blog.naver.com/sws040201)


## 채연:
### 목표: 
EBM에 대해 알아본다

### 결과:

#### 정의:
에너지 기반 모델(EBM, Energy-Based Model)은 머신러닝과 통계물리학의 중요한 개념을 바탕으로 한 모델. EBM의 핵심 아이디어는 뉴럴 네트워크가 어떤 스칼라 값을 출력하고, 이 값이 볼츠만 분포(Boltzmann distribution: 해당 상태의 에너지와 온도의 함수로 특정 상태 에 있을 확률을 제공 하는 확률 분포 )를 따르는 방식으로 로스를 정의한다는 점이다.

#### 장점:

범용성 EBM은 특정 모델 구조에 국한되지 않고 다양한 모델에 적용할 수 있습니다. 오토인코더(AE), 정규 흐름(NF) 등 다양한 머신러닝 모델과 조합하여 사용할 수 있다. loss 의 형태가 중요하기 때문에 모델 설계의 유연성이 높다.

다양한 트레이닝 방식 Positive/Negative 에너지 그라디언트: 긍정/부정 에너지의 차이를 줄이는 방식으로 트레이닝이 가능하다. Positive/Partition 함수: 볼츠만 분포의 정의를 사용하여 직접 학습하는 방식이다. Score Matching: 데이터의 기울기를 맞추는 방식으로 트레이닝을 수행할 수 있다.

이러한 다양한 학습 방식은 EBM의 트레이닝을 상황에 맞게 최적화할 수 있게 해준다.

샘플 생성의 용이성 EBM을 사용하면 샘플 생성이 비교적 용이하다. 해밀토니안 몬테카를로(HMC)나 랑주뱅 다이내믹스(Langevin dynamics)와 같은 방법을 통해 효과적으로 샘플을 생성할 수 있다. 이로 인해 모델이 데이터 분포를 잘 학습하고 새로운 데이터를 생성할 수 있는 능력이 향상된다.

에너지 기반 해석 가능성 모델의 출력 값을 에너지로 해석할 수 있으므로, 모델의 동작을 이해하고 설명하는 데 용이하다. 이는 모델의 투명성과 신뢰성을 높이는 데 기여한다.

확률 분포 학습 EBM은 데이터의 확률 분포를 직접 학습할 수 있다. 복잡한 데이터 분포를 효과적으로 모델링할 수 있게 해주며, 생성 모델로서의 성능을 강화한다.

불확실성 평가 EBM은 모델 출력의 에너지 값을 통해 불확실성을 평가할 수 있다. 모델이 예측할 때 어느 정도의 신뢰도를 갖는지 평가할 수 있게 해준다.

트레이닝 안정성 다양한 트레이닝 방법을 통해 모델의 안정성을 높일 수 있으며, 에너지 기반 그라디언트 방법은 안정적인 학습을 가능하게 한다. 기존 모델과의 통합

EBM은 기존의 딥러닝 모델과 통합하여 성능을 향상시킬 수 있습니다. 이는 EBM이 다양한 애플리케이션에 적용될 수 있는 중요한 이유 중 하나입니다.

#### 단점:

높은 계산 비용 에너지 기반 모델의 학습과 샘플링 과정은 계산 비용이 매우 높을 수 있다. 특히 해밀토니안 몬테카를로(HMC)나 랑주뱅 다이내믹스(Langevin dynamics)와 같은 샘플링 기법은 많은 계산 자원을 요구한다.

```
해밀토니안 몬테카를로 (Hamiltonian Monte Carlo, HMC)
물리학의 해밀토니안 역학(Hamiltonian dynamics)을 기반으로 한 마코프 체인 몬테카를로(MCMC) 방법. 
고차원 확률 분포의 샘플링을 개선하기 위해 설계되었다.
장점
효율적인 샘플링: 샘플링 경로를 길게 만들어 샘플 간의 상관성을 줄이고, 고차원 공간에서도 효율적으로 샘플링할 수 있다.
빠른 수렴: 에너지 경사를 이용해 샘플을 이동시키므로, 분포의 고차원 공간에서 빠르게 수렴할 수 있다.
단점
복잡한 구현: 해밀토니안 역학과 보존 법칙을 이용한 복잡한 수학적 개념을 필요로 하므로 구현이 어려울 수 있다.
조정이 어렵다: Leapfrog 스텝 사이즈와 같은 하이퍼파라미터를 잘 조정해야 효과적인 샘플링이 가능하다.
과정
잠재 변수 생성: 대상 변수에 대해 잠재 변수를 도입
해밀토니안 설정: 잠재 변수와 대상 변수의 결합 분포를 나타내는 해밀토니안을 설정
Leapfrog 통합: 해밀토니안 역학을 따라 잠재 변수와 대상 변수를 업데이트
메트로폴리스-헤이스팅스 알고리즘: 새로운 샘플을 수락하거나 거부하는 기준을 적용


랑주뱅 다이내믹스 (Langevin Dynamics)

확률론적 접근 방식으로, 대상 확률 분포에 노이즈를 추가하여 샘플을 생성하는 방법으로, 확률적 경사 하강법(Stochastic Gradient Descent, SGD)의 확장으로 볼 수 있다.

장점
단순한 구현: HMC에 비해 수학적 개념이 덜 복잡하여 구현이 비교적 쉽다.
노이즈 추가: 모델에 노이즈를 추가하여 다양한 샘플을 생성할 수 있다.
단점
효율성 저하: HMC에 비해 고차원 공간에서 샘플링 효율이 떨어질 수 있다.
적절한 스텝 사이즈 필요: 너무 큰 스텝 사이즈는 정확한 샘플링을 방해하고, 너무 작은 스텝 사이즈는 수렴 속도를 느리게 한다.
과정
초기화: 샘플 초기값을 설정한다.
확률적 경사 계산: 현재 샘플 위치에서 확률적 경사를 계산한다.
노이즈 추가: 샘플 이동 시 노이즈를 추가한다.
샘플 업데이트: 경사와 노이즈를 사용하여 샘플을 업데이트한다.


HMC는 해밀토니안 역학을 활용하여 고차원 분포에서 효율적으로 샘플링할 수 있지만, 구현과 조정이 어려울 수 있다.
Langevin dynamics는 구현이 비교적 쉬운 대신 고차원에서의 샘플링 효율이 떨어질 수 있다.
```

학습의 어려움 에너지 함수의 최적화를 위한 학습 과정은 복잡하고 까다로울 수 있습니다. 특히, 학습 과정에서 발생하는 수렴 문제나 에너지 경사 하강법의 불안정성은 모델 학습을 어렵게 할 수 있다.

정규화 상수 계산의 어려움 에너지 기반 모델에서 정규화 상수(Z)를 계산하는 것은 매우 어려운 문제이다. 이 상수는 분포를 정상화하는 데 필요하지만, 고차원 공간에서는 계산이 거의 불가능할 수 있다.

모델 평가의 복잡성 에너지 기반 모델의 성능을 평가하는 것은 다른 모델에 비해 더 복잡할 수 있다. 에너지 함수의 값만으로는 모델의 품질을 직관적으로 평가하기 어려울 수 있다.

샘플링의 어려움 샘플링 과정이 복잡하고 시간이 많이 소요될 수 있다. 특히 고차원 데이터에서는 효율적인 샘플링이 어려워질 수 있다.

해석의 어려움 에너지 함수의 형태나 모델의 구조가 복잡한 경우, 모델의 동작을 해석하고 이해하는 것이 어려울 수 있다. 이는 모델의 투명성과 설명 가능성을 저하시킬 수 있다.

하이퍼파라미터 튜닝의 복잡성 EBM은 여러 하이퍼파라미터를 가지고 있으며, 이들의 적절한 값을 찾는 것이 까다로울 수 있다. 잘못된 하이퍼파라미터 설정은 모델의 성능에 큰 영향을 미칠 수 있다.

학습 데이터의 의존성 EBM은 충분한 양의 고품질 학습 데이터가 필요하다. 데이터가 부족하거나 품질이 낮은 경우, 모델의 성능이 크게 저하될 수 있다. 이러한 단점들은 EBM을 실제로 적용할 때 고려해야 할 중요한 요소들이다. 모델의 강점을 최대한 활용하면서도 단점을 보완하는 방법을 찾는 것이 성공적인 EBM 응용의 핵심이다.


### 인증샷:
![[team-blog-코딩황제들-2024-week5.모각코5일차회의인증.png]]
![[team-blog-코딩황제들-2024-week5.모각코5일차시간인증.jpg]]