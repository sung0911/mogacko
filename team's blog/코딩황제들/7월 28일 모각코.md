# 계획
프로젝트의 목표에 맞는 데이터 특성에 가장 적합한 손실 함수를 찾기 위해 다양한 손실함수에 대해 공부한다. 각 로스함수의 수학적 정의 및 유도과정 이해, 각 로스함수가 사용되는 모델과 그 이유(어떤 모델과 문제에 적합한지), 장 단점 분석, 간단한 구현예제를 작성해보면서 이해해본다. 또한 다양한 학습방법 및 디퓨전모델에 대해서도 알아본다.


**회의방법**
온라인(naver whale on) *zoom은 40분이상하려면 유료로 결제를 해야하기 때문에 whale on을 활용했습니다. 

**팀원 블로그**
박세준 https://kepler-dev-3141.github.io/
신우석 https://blog.naver.com/sws040201/
김채연 https://kcyeon0127.github.io/


# 결과

## 준혁:
### 목표: 
Wasserstein Loss를 사용한 GAN 예제 코드를 작성해보면서 이를 통해 GAN의 기본 개념과 WGAN의 작동 방식을 이해해본다

### 결과:
이 WGAN 예제는 단순히 1차원 정규 분포 데이터를 생성자와 판별자가 학습하도록 설계를 하였습니다.

[WGAN  example.ipynb - Colab (google.com)](https://colab.research.google.com/drive/1aHY4LrJE3tFZJ_ZwJFUZ5qGxkTtHY0fG#scrollTo=wex4SWI7G0qU&uniqifier=1)

- ### 코드 설명

1. **라이브러리 임포트**:
    
    - `numpy`와 `tensorflow` 라이브러리를 임포트합니다.
2. **Wasserstein 손실 함수 정의**:
    
    - `wasserstein_loss(y_true, y_pred)`: 실제 값과 예측 값의 곱의 평균을 계산하는 손실 함수입니다.
3. **생성자 모델 정의**:
    
    - `build_generator()`: 단순한 두 개의 Dense 레이어로 구성된 생성자 모델입니다.
    - 첫 번째 레이어는 16개의 노드를 사용하고, 두 번째 레이어는 단일 값을 출력합니다.
4. **판별자 모델 정의**:
    
    - `build_discriminator()`: 단순한 두 개의 Dense 레이어로 구성된 판별자 모델입니다.
    - 첫 번째 레이어는 16개의 노드를 사용하고, 두 번째 레이어는 단일 값을 출력합니다.
5. **데이터 생성 함수**:
    
    - `generate_real_data(samples)`: 지정된 샘플 수만큼 1차원 정규 분포 데이터를 생성합니다.
6. **훈련 함수**:
    
    - `train(generator, discriminator, epochs, batch_size, latent_dim)`: 생성자와 판별자를 훈련시키는 함수입니다.
    - 각 에포크마다 판별자를 훈련시키고, 그 다음 생성자를 훈련시킵니다.
7. **파라미터 설정**:
    
    - `latent_dim`: 잠재 공간의 차원
    - `samples`: 전체 샘플 수
    - `batch_size`: 배치 크기
    - `epochs`: 학습 에포크 수
8. **모델 빌드 및 컴파일**:
    
    - 생성자와 판별자 모델을 빌드하고, `RMSprop` 옵티마이저와 함께 `wasserstein_loss` 손실 함수를 사용하여 컴파일합니다.
9. **GAN 모델 빌드 및 컴파일**:
    
    - 판별자의 가중치를 동결하고, 생성자와 판별자를 결합하여 GAN 모델을 빌드합니다.
    - 결합된 모델을 `RMSprop` 옵티마이저와 함께 `wasserstein_loss` 손실 함수를 사용하여 컴파일합니다.
10. **모델 훈련**:
    
    - `train(generator, discriminator, epochs, batch_size, latent_dim)`: GAN 모델을 훈련합니다.



## 세준:
### 목표: 


### 결과:


## 우석:
### 목표:
softmax에 대해 알아본다.


### 결과 :

logit은 log odds를 뜻한다. odds는 얻을 확률과 잃을 확률의 비 라고 생각하면 된다.

odds에서 얻을 확률을 y라고 하면 잃을 확률은 1-y가 된다.

Classes : C1, C2

P(C1|X) : y, P(C2|X) : 1-y

$odds\ =\ \frac{y}{1-y}=\frac{P\left(\combi{C}_1\mid X\right)}{1-P\left(\combi{C}_1\mid X\right)}$odds = y1−y​=P(C1​∣X)1−P(C1​∣X)​​

​

신경망(Neural Network, NN)을 생각할 때, 출력층의 값 z를 계산하는 방식은

$z\ =\ \combi{\theta }_0+\combi{\theta }_1\combi{x}_1+\combi{\theta }_2\combi{x}_2+\cdot \cdot \cdot $z = θ0​+θ1​x1​+θ2​x2​+···​

θ는 신경망의 가중치이고, x는 입력 값이다. 범위는 -∞<z<∞ 이다.

log(odds)또한 odds에 로그를 취한 것이므로 범위는 같다.

​

logit과 z의 범위가 같으니 logit을 z로 두고 식을 전개한다.

$z\ =\ \log \left(\frac{y}{1-y}\right),\ \combi{e}^z=\left(\frac{y}{1-y}\right)$z = log(y1−y​), ez=(y1−y​)​

$y\ =\ \frac{\combi{e}^z}{1+\combi{e}^z}$y = ez1+ez​​

이 식은 sigmoid 함수와 동일하다. 여기서 e^-z를 분모와 분자에 곱해주면 같아진다.

따라서 sigmoid와 logit은 역함수 관계임을 알 수 있다.

$sigmoid\ =\ \frac{1}{1+\combi{\e }^{-z}}$sigmoid = 11+℮−z​​

​softmax는 sigmoid 함수를 다중 클래스 분류로 일반화하면 유도할 수 있다.

**[출처]** [8/01](https://blog.naver.com/sws040201/223533552641)|**작성자** [sws040201](https://blog.naver.com/sws040201)


### 결과:


## 채연:
### 목표: 
Binary Cross-Entropy (BCE) Loss의 수학적으로 이해해본다.



### 결과:

Binary Cross-Entropy (BCE) Loss의 수식을 수학적으로 설명하겠습니다. BCE Loss는 이진 분류 문제에서 모델의 예측 확률과 실제 라벨 간의 차이를 측정하는 손실 함수입니다. 이를 통해 모델의 성능을 평가하고, 최적화할 수 있습니다.

BCE Loss:

[ L = - 1/N {sum_i=1~N} [ y_i * log(p_i) + (1 - y_i) * log(1 - p_i) ]

- ( L )은 손실 함수의 값
- ( N )은 총 샘플 수
- ( y_i )는 i번째 샘플의 실제 라벨(이진 분류 문제에서는 0 또는 1)
- ( p_i )는 i번째 샘플이 클래스 1일 확률로 모델이 예측한 값 (0과 1 사이의 값)

1. **로그항 (( log(p_i) )와 ( log(1 - p_i) ))**:
    - 로그는 정보 이론에서 엔트로피를 계산할 때 사용됩니다. 여기서 로그를 사용하는 이유는 예측 확률이 낮을 때 페널티를 크게 주기 위함입니다.
    - ( \log(p_i) ): 모델이 클래스 1일 확률을 예측한 값에 로그를 취한다.
    - ( \log(1 - p_i) ): 모델이 클래스 0일 확률을 예측한 값에 로그를 취합니다.

( p_i )가 1에 가까워질수록 ( \log(p_i) )는 0에 가까워지고, ( p_i )가 0에 가까워질수록 ( \log(p_i) )는 음의 무한대로 커진다. 로그 함수를 사용하면 예측 확률이 낮을 때 손실 값이 급격히 커지게 된다. 이는 모델이 잘못된 예측을 할 경우 큰 페널티를 부과하여, 모델이 더 정확한 예측을 하도록 유도한다.

1. **실제 라벨 (( y_i ))에 따른 조건부 손실**:
    - ( y_i = 1 )일 때, 손실 항목은 ( \log(p_i) )가 된다. 이는 모델이 클래스 1일 확률을 얼마나 잘 예측했는지를 나타낸다.
    - ( y_i = 0 )일 때, 손실 항목은 ( \log(1 - p_i) )가 된다. 이는 모델이 클래스 0일 확률을 얼마나 잘 예측했는지를 나타낸다.
2. **전체 손실 계산**:
    - ( -[ y_i * log(p_i) + (1 - y_i) * log(1 - p_i) ] )는 i번째 샘플의 손실을 계산한다.
    - 각 샘플의 손실을 모두 더한 후, ( N )으로 나누어 평균 손실을 구한다. 이는 샘플 수에 관계없이 일관된 손실 값을 제공한다.
3. **부호**:
    - 확률p가 0.5보다 작으면 로그 값은 음수가 되기 때문에 해석하기 어려워질 수 있고, 해석이 일관되게 하기 위해서, BCE Loss는 로그 값의 음수를 취하여 손실 값을 양수로 만든다. ( 손실 값을 양수로 만들기 위해 전체에 -1을 곱합니다.)

BCE Loss는 모델이 예측한 확률 ( p_i )와 실제 라벨 ( y_i ) 간의 차이를 로그 함수와 결합하여 측정하며, 이를 통해 모델이 얼마나 잘 예측하는지를 평가한다. 이 손실 함수를 최소화함으로써 모델의 성능을 최적화할 수 있다.



### 인증샷:

![[team-blog-코딩황제들-2024-week3.모각코3일차회의인증.png]]
![[team-blog-코딩황제들-2024-week3.모각코3일차시간인증.jpg]]
