# 계획
프로젝트의 목표에 맞는 데이터 특성에 가장 적합한 손실 함수를 찾기 위해 다양한 손실함수에 대해 공부한다. 각 로스함수의 수학적 정의 및 유도과정 이해, 각 로스함수가 사용되는 모델과 그 이유(어떤 모델과 문제에 적합한지), 장 단점 분석, 간단한 구현예제를 작성해보면서 이해해본다. 또한 다양한 학습방법 및 디퓨전모델에 대해서도 알아본다.

**회의방법**
온라인(naver whale on) *zoom은 40분이상하려면 유료로 결제를 해야하기 때문에 whale on을 활용했습니다. 

**팀원 블로그**
박세준 https://kepler-dev-3141.github.io/
신우석 https://blog.naver.com/sws040201/
김채연 https://kcyeon0127.github.io/

**ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ--------**
# 결과

## 준혁(팀장)
### 목표: 
Wasserstein Loss를 사용한 GAN 예제 코드를 작성해보면서 이를 통해 GAN의 기본 개념과 WGAN의 작동 방식을 이해해본다

### 결과:
이 WGAN 예제는 단순히 1차원 정규 분포 데이터를 생성자와 판별자가 학습하도록 설계를 하였습니다.

[WGAN  example.ipynb - Colab (google.com)](https://colab.research.google.com/drive/1aHY4LrJE3tFZJ_ZwJFUZ5qGxkTtHY0fG#scrollTo=wex4SWI7G0qU&uniqifier=1)

- ### 코드 설명

1. **라이브러리 임포트**:
    
    - `numpy`와 `tensorflow` 라이브러리를 임포트합니다.
2. **Wasserstein 손실 함수 정의**:
    
    - `wasserstein_loss(y_true, y_pred)`: 실제 값과 예측 값의 곱의 평균을 계산하는 손실 함수입니다.
3. **생성자 모델 정의**:
    
    - `build_generator()`: 단순한 두 개의 Dense 레이어로 구성된 생성자 모델입니다.
    - 첫 번째 레이어는 16개의 노드를 사용하고, 두 번째 레이어는 단일 값을 출력합니다.
4. **판별자 모델 정의**:
    
    - `build_discriminator()`: 단순한 두 개의 Dense 레이어로 구성된 판별자 모델입니다.
    - 첫 번째 레이어는 16개의 노드를 사용하고, 두 번째 레이어는 단일 값을 출력합니다.
5. **데이터 생성 함수**:
    
    - `generate_real_data(samples)`: 지정된 샘플 수만큼 1차원 정규 분포 데이터를 생성합니다.
6. **훈련 함수**:
    
    - `train(generator, discriminator, epochs, batch_size, latent_dim)`: 생성자와 판별자를 훈련시키는 함수입니다.
    - 각 에포크마다 판별자를 훈련시키고, 그 다음 생성자를 훈련시킵니다.
7. **파라미터 설정**:
    
    - `latent_dim`: 잠재 공간의 차원
    - `samples`: 전체 샘플 수
    - `batch_size`: 배치 크기
    - `epochs`: 학습 에포크 수
8. **모델 빌드 및 컴파일**:
    
    - 생성자와 판별자 모델을 빌드하고, `RMSprop` 옵티마이저와 함께 `wasserstein_loss` 손실 함수를 사용하여 컴파일합니다.
9. **GAN 모델 빌드 및 컴파일**:
    
    - 판별자의 가중치를 동결하고, 생성자와 판별자를 결합하여 GAN 모델을 빌드합니다.
    - 결합된 모델을 `RMSprop` 옵티마이저와 함께 `wasserstein_loss` 손실 함수를 사용하여 컴파일합니다.
10. **모델 훈련**:
    
    - `train(generator, discriminator, epochs, batch_size, latent_dim)`: GAN 모델을 훈련합니다.

**ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ--------**
## 세준
### 목표
Diffusion Model에 대해 추가적으로 알아보기

### 결과

![[team-blog-코딩황제들-2024-week3.세준1.png]]

#### 1. Reverse process

Reverse process는 로부터 으로 복원하는 과정을 말한다.  랜덤 노이즈로부터 데이터를 생성하는 모델로 사용되기 때문에 필수적이지만 이것을 실제로 알아내기는 쉽지 않다. 따라서 를 활용해서 근사화한다. 이 근사화 과정은 Gaussian transition을 활용한 Markov chain의 형태를 가진다.

노이즈 에서 원래 데이터 로 Reverse process를 통해 가는 공동 확률 분포를 나타낸다.

- : 시간 단계 에서 데이터의 사전 분포.
- : 역과정의 각 단계 에서 에서 로 가는 조건부 확률 분포를 나타낸다.

이 수식은 각 역과정 단계의 조건부 확률 분포의 형태를 지정한다. 역과정은 가우시안 분포로 모델링된다.

- : 가우시안 분포의 평균으로, 현재 상태 와 시간 단계 의 함수이다.
- : 가우시안 분포의 공분산 행렬로, 현재 상태 ​와 시간 단계 의 함수이다.

위 식에서, 각 단계의 정규 분포의 평균 와 표준편차 는 학습되어야 하는 파라미터이다. 시작지점의 노이즈 분포는 표준정규분포로 정의한다.

#### 2. Forward process

 Forward process q는 data()으로부터 noise를 더해가면서 최종 노이즈() 형태로 가는 과정이다.  
Reverse process의 학습을 Forward process의 정보를 활용해서 하기 때문에 이 과정의 분포를 알아야 한다. data에 Gaussian noise를 조금씩 더하는 Markov chain의 형태를 가진다.

**ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ--------**
## 우석
### 목표:
softmax에 대해 알아본다.
### 결과 :

logit은 log odds를 뜻한다. odds는 얻을 확률과 잃을 확률의 비 라고 생각하면 된다.

odds에서 얻을 확률을 y라고 하면 잃을 확률은 1-y가 된다.

Classes : C1, C2

P(C1|X) : y, P(C2|X) : 1-y

$odds\ =\ \frac{y}{1-y}=\frac{P\left({C}_1\mid X\right)}{1-P\left({C}_1\mid X\right)}$​​

​신경망(Neural Network, NN)을 생각할 때, 출력층의 값 z를 계산하는 방식은

$z\ =\ {\theta }_0+{\theta }_1{x}_1+{\theta }_2{x}_2+\cdot \cdot \cdot$

θ는 신경망의 가중치이고, x는 입력 값이다. 범위는 -∞<z<∞ 이다.

log(odds)또한 odds에 로그를 취한 것이므로 범위는 같다.

​logit과 z의 범위가 같으니 logit을 z로 두고 식을 전개한다.

$z\ =\ \log \left(\frac{y}{1-y}\right),\ {e}^z=\left(\frac{y}{1-y}\right)$z = log(y1−y​), ez=(y1−y​)​

$y\ =\ \frac{{e}^z}{1+{e}^z}$y = ez1+ez​​

이 식은 sigmoid 함수와 동일하다. 여기서 e^-z를 분모와 분자에 곱해주면 같아진다.

따라서 sigmoid와 logit은 역함수 관계임을 알 수 있다.

$sigmoid\ =\ \frac{1}{1+{e }^{-z}}$​​

​softmax는 sigmoid 함수를 다중 클래스 분류로 일반화하면 유도할 수 있다.

**[출처]** [8/01](https://blog.naver.com/sws040201/223533552641)|**작성자** [sws040201](https://blog.naver.com/sws040201)

**ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ--------**
## 채연
### 목표: 
Binary Cross-Entropy (BCE) Loss의 수학적으로 이해해본다.

### 결과:

Binary Cross-Entropy (BCE) Loss의 수식을 수학적으로 설명하겠습니다. BCE Loss는 이진 분류 문제에서 모델의 예측 확률과 실제 라벨 간의 차이를 측정하는 손실 함수입니다. 이를 통해 모델의 성능을 평가하고, 최적화할 수 있습니다.

BCE Loss:

$L = - 1/N {sum_i=1~N}  y_i * log(p_i) + (1 - y_i) * log(1 - p_i)$

- ( L )은 손실 함수의 값
- ( N )은 총 샘플 수
- ( y_i )는 i번째 샘플의 실제 라벨(이진 분류 문제에서는 0 또는 1)
- ( p_i )는 i번째 샘플이 클래스 1일 확률로 모델이 예측한 값 (0과 1 사이의 값)

1. **로그항 (( log(p_i) )와 ( log(1 - p_i) ))**:
    - 로그는 정보 이론에서 엔트로피를 계산할 때 사용됩니다. 여기서 로그를 사용하는 이유는 예측 확률이 낮을 때 페널티를 크게 주기 위함입니다.
    - ( \log(p_i) ): 모델이 클래스 1일 확률을 예측한 값에 로그를 취한다.
    - ( \log(1 - p_i) ): 모델이 클래스 0일 확률을 예측한 값에 로그를 취합니다.

( p_i )가 1에 가까워질수록 ( \log(p_i) )는 0에 가까워지고, ( p_i )가 0에 가까워질수록 ( \log(p_i) )는 음의 무한대로 커진다. 로그 함수를 사용하면 예측 확률이 낮을 때 손실 값이 급격히 커지게 된다. 이는 모델이 잘못된 예측을 할 경우 큰 페널티를 부과하여, 모델이 더 정확한 예측을 하도록 유도한다.

1. **실제 라벨 (( y_i ))에 따른 조건부 손실**:
    - ( y_i = 1 )일 때, 손실 항목은 ( \log(p_i) )가 된다. 이는 모델이 클래스 1일 확률을 얼마나 잘 예측했는지를 나타낸다.
    - ( y_i = 0 )일 때, 손실 항목은 ( \log(1 - p_i) )가 된다. 이는 모델이 클래스 0일 확률을 얼마나 잘 예측했는지를 나타낸다.
2. **전체 손실 계산**:
    - $y_i * log(p_i) + (1 - y_i) * log(1 - p_i)$는 i번째 샘플의 손실을 계산한다.
    - 각 샘플의 손실을 모두 더한 후, ( N )으로 나누어 평균 손실을 구한다. 이는 샘플 수에 관계없이 일관된 손실 값을 제공한다.
3. **부호**:
    - 확률p가 0.5보다 작으면 로그 값은 음수가 되기 때문에 해석하기 어려워질 수 있고, 해석이 일관되게 하기 위해서, BCE Loss는 로그 값의 음수를 취하여 손실 값을 양수로 만든다. ( 손실 값을 양수로 만들기 위해 전체에 -1을 곱합니다.)

BCE Loss는 모델이 예측한 확률 ( p_i )와 실제 라벨 ( y_i ) 간의 차이를 로그 함수와 결합하여 측정하며, 이를 통해 모델이 얼마나 잘 예측하는지를 평가한다. 이 손실 함수를 최소화함으로써 모델의 성능을 최적화할 수 있다.

**ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ--------**
## 인증샷

![[team-blog-코딩황제들-2024-week3.모각코3일차회의인증.png]]
![[team-blog-코딩황제들-2024-week3.모각코3일차시간인증.jpg]]
