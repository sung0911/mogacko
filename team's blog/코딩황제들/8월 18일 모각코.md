# 계획
energy-based model, 특히 energy-based GAN에 대해 알아보기 및 대조학습코드 구현 

**회의방법**
온라인(naver whale on) *zoom은 40분이상하려면 유료로 결제를 해야하기 때문에 whale on을 활용했습니다. 

**팀원 블로그**
박세준 https://kepler-dev-3141.github.io/
신우석 https://blog.naver.com/sws040201/
김채연 https://kcyeon0127.github.io/

**ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ------------**
# 결과

## 준혁(팀장)
### 목표: 
에너지 기반 GAN(Energy-Based Generative Adversarial Network, EBGAN)에 대해 알아본다.
### 결과:

#### 1. EBGAN이란?
*전통적인 GAN모델의 변형으로, 에너지 기반 모델(EBM)의 개념을 도입하여 GAN의 판별자(Discriminator)를 재설계한 모델입니다. EBGAN은 전통적인 GAN보다 더 안정적이고 유연한 학습을 가능하게 하기 위해 제안되었습니다.

#### 2. **전통적인 GAN의 기본 개념**

- **생성자(Generator):** 랜덤한 노이즈 벡터를 입력으로 받아, 이로부터 현실적인 데이터를 생성하는 네트워크입니다.
- **판별자(Discriminator):** 입력 데이터가 실제(real) 데이터인지 생성된(fake) 데이터인지 구분하는 네트워크입니다.
- GAN의 목표는 생성자가 점점 더 현실적인 데이터를 생성하도록 학습하는 것입니다. 생성자가 만든 데이터가 실제 데이터를 닮아가면서, 판별자는 점점 더 어려운 구별 문제를 해결해야 하므로 두 네트워크가 서로 경쟁하면서 발전합니다.

#### 3. **EBGAN에서의 변화**

- **에너지 기반 판별자:** EBGAN에서 판별자는 단순히 이진 분류를 수행하는 대신, 데이터를 입력받아 그 데이터의 "에너지"를 계산합니다. 이 에너지는 데이터가 얼마나 "현실적인지"를 나타내며, EBM에서 사용되는 에너지 함수와 유사한 역할을 합니다.
- **목표:** EBGAN의 목표는 생성된 데이터에 높은 에너지를, 실제 데이터에 낮은 에너지를 할당하는 것입니다.

#### 4. **EBGAN의 작동 방식**

- **판별자의 역할:** EBGAN에서 판별자는 입력 데이터 xxx에 대해 에너지 값 E(x)E(x)E(x)를 계산합니다. 이 에너지는 데이터가 실제에 가까울수록 낮게, 비현실적일수록 높게 설정됩니다.
- **생성자의 역할:** 생성자는 낮은 에너지를 가진 데이터를 생성하도록 학습합니다. 즉, 판별자가 낮은 에너지를 할당하는 데이터를 생성하려고 시도합니다.
- **손실 함수:** EBGAN의 손실 함수는 전통적인 GAN과 다르게, 판별자가 계산한 에너지 값을 최소화하거나 최대화하는 방식으로 정의됩니다.

#### 5. **EBGAN의 장점**

- **학습의 안정성:** 전통적인 GAN에서는 종종 학습이 불안정해지거나 모드 붕괴(mode collapse)가 발생할 수 있습니다. EBGAN은 에너지 기반 접근 방식을 통해 이러한 문제를 완화할 수 있습니다.
- **유연성:** 판별자가 에너지를 계산하는 방식은 단순한 이진 분류보다 더 유연하여, 다양한 형태의 손실 함수나 학습 전략을 적용할 수 있습니다.

#### 6. **예시**

- 예를 들어, 이미지 생성 문제에서 EBGAN은 판별자가 이미지에 대해 "에너지"를 계산하고, 생성자는 이 에너지를 낮추는 방향으로 이미지를 생성합니다. 결과적으로, 생성된 이미지는 더 현실적이고, 판별자는 더 까다로운 기준으로 이미지를 평가하게 됩니다.

#### 7. **EBGAN의 응용**

- **이미지 생성:** 현실적인 이미지를 생성하는 데 사용할 수 있습니다. 특히, 고해상도 이미지 생성이나 다양한 스타일의 이미지 생성에 효과적일 수 있습니다.
- **비디오 생성:** 비디오의 연속적인 프레임을 생성할 때, 각 프레임이 이전 프레임과의 연속성을 가지면서도 개별적으로 현실적인 에너지를 가지도록 학습할 수 있습니다.
- **자연어 처리:** 텍스트 생성에서도 EBGAN을 응용하여 더 자연스러운 문장을 생성하는 데 사용될 수 있습니다.

**ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ------------**
## 세준
### 목표: 

저번에 구현해 본 EBGAN이 왜 잘 되지 않았는지 분석하기

### 결과:

Github에서 EBGAN이 구현된 코드를 실행해보고 내가 구현한 코드와 어떻게 다른지 분석해보기로 하였다.  
[https://github.com/znxlwm/pytorch-generative-model-collections](https://github.com/znxlwm/pytorch-generative-model-collections)  
Github에 있는 코드는 Colab에서 작동되도록 구현되지 않았고 결과를 실시간으로 볼 수 없어서 코드를 수정하였다.

```python
import torch, time, os, pickle
import numpy as np
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import imageio
import gzip
import scipy.misc
import matplotlib.pyplot as plt
```

```python
from google.colab import drive
drive.mount('/content/drive')
```

```python
def print_network(net):
    num_params = 0
    for param in net.parameters():
        num_params += param.numel()
    print(net)
    print('Total number of parameters: %d' % num_params)
  
def initialize_weights(net):
    for m in net.modules():
        if isinstance(m, nn.Conv2d):
            m.weight.data.normal_(0, 0.02)
            m.bias.data.zero_()
        elif isinstance(m, nn.ConvTranspose2d):
            m.weight.data.normal_(0, 0.02)
            m.bias.data.zero_()
        elif isinstance(m, nn.Linear):
            m.weight.data.normal_(0, 0.02)
            m.bias.data.zero_()
def dataloader(input_size, batch_size, split='train'):
    transform = transforms.Compose([transforms.Grayscale(), transforms.Resize((input_size, input_size)), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
    data_loader = DataLoader(
        datasets.MNIST('data/mnist', train=True, download=True, transform=transform),
        batch_size=batch_size, shuffle=True)
  
    return data_loader
```

```python
class generator(nn.Module):
    # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)
    # Architecture : FC1024_BR-FC7x7x128_BR-(64)4dc2s_BR-(1)4dc2s_S
    def __init__(self, input_dim, output_dim, input_size):
        super(generator, self).__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.input_size = input_size
  
        self.fc = nn.Sequential(
            nn.Linear(self.input_dim, 1024),
            nn.BatchNorm1d(1024),
            nn.ReLU(),
            nn.Linear(1024, 128 * (self.input_size // 4) * (self.input_size // 4)),
            nn.BatchNorm1d(128 * (self.input_size // 4) * (self.input_size // 4)),
            nn.ReLU(),
        )
        self.deconv = nn.Sequential(
            nn.ConvTranspose2d(128, 64, 4, 2, 1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.ConvTranspose2d(64, self.output_dim, 4, 2, 1),
            nn.Tanh(),
        )
        initialize_weights(self)
  
    def forward(self, input):
        x = self.fc(input)
        x = x.view(-1, 128, (self.input_size // 4), (self.input_size // 4))
        x = self.deconv(x)
  
        return x
```

```python
class discriminator(nn.Module):
    # It must be Auto-Encoder style architecture
    # Architecture : (64)4c2s-FC32-FC64*14*14_BR-(1)4dc2s_S
    def __init__(self, input_dim, output_dim, input_size):
        super(discriminator, self).__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.input_size = input_size
  
        self.conv = nn.Sequential(
            nn.Conv2d(self.input_dim, 64, 4, 2, 1),
            nn.ReLU(),
        )
        self.code = nn.Sequential(
            nn.Linear(64 * (self.input_size // 2) * (self.input_size // 2), 32), # bn and relu are excluded since code is used in pullaway_loss
        )
        self.fc = nn.Sequential(
            nn.Linear(32, 64 * (self.input_size // 2) * (self.input_size // 2)),
            nn.BatchNorm1d(64 * (self.input_size // 2) * (self.input_size // 2)),
            nn.ReLU(),
        )
        self.deconv = nn.Sequential(
            nn.ConvTranspose2d(64, self.output_dim, 4, 2, 1),
            # nn.Sigmoid(),
        )
        initialize_weights(self)
  
    def forward(self, input):
        x = self.conv(input)
        x = x.view(x.size()[0], -1)
        code = self.code(x)
        x = self.fc(code)
        x = x.view(-1, 64, (self.input_size // 2), (self.input_size // 2))
        x = self.deconv(x)
  
        return x, code
```

```python
class EBGAN(object):
    def __init__(self):
        # parameters
        self.epoch = 50
        self.sample_num = 100
        self.batch_size = 64
        self.save_dir = '/content/drive/MyDrive/EBGAN/model/'
        self.result_dir = '/content/drive/MyDrive/EBGAN/res/'
        self.log_dir = '/content/drive/MyDrive/EBGAN/log/'
        self.gpu_mode = True
        self.model_name = 'EBGAN'
        self.input_size = 28
        self.z_dim = 62
        self.pt_loss_weight = 0.1
        self.margin = 1
        self.dataset = 'mnist'
  
        self.lrG = 0.0002
        self.lrD = 0.0002
        self.beta1 = 0.5
        self.beta2 = 0.999
  
        # load dataset
        self.data_loader = dataloader(self.input_size, self.batch_size)
        data = self.data_loader.__iter__().__next__()[0]
  
        # networks init
        self.G = generator(input_dim=self.z_dim, output_dim=data.shape[1], input_size=self.input_size)
        self.D = discriminator(input_dim=data.shape[1], output_dim=1, input_size=self.input_size)
        self.G_optimizer = optim.Adam(self.G.parameters(), lr=self.lrG, betas=(self.beta1, self.beta2))
        self.D_optimizer = optim.Adam(self.D.parameters(), lr=self.lrD, betas=(self.beta1, self.beta2))
  
        if self.gpu_mode:
            self.G.cuda()
            self.D.cuda()
            self.MSE_loss = nn.MSELoss().cuda()
        else:
            self.MSE_loss = nn.MSELoss()
  
        print('---------- Networks architecture -------------')
        print_network(self.G)
        print_network(self.D)
        print('-----------------------------------------------')
  
        # fixed noise
        self.sample_z_ = torch.rand((self.batch_size, self.z_dim))
        if self.gpu_mode:
            self.sample_z_ = self.sample_z_.cuda()
  
    def train(self):
        self.train_hist = {}
        self.train_hist['D_loss'] = []
        self.train_hist['G_loss'] = []
        self.train_hist['per_epoch_time'] = []
        self.train_hist['total_time'] = []
  
        self.y_real_, self.y_fake_ = torch.ones(self.batch_size, 1), torch.zeros(self.batch_size, 1)
        if self.gpu_mode:
            self.y_real_, self.y_fake_ = self.y_real_.cuda(), self.y_fake_.cuda()
  
        self.D.train()
        print('training start!!')
        start_time = time.time()
        for epoch in range(self.epoch):
            self.G.train()
            epoch_start_time = time.time()
            for iter, (x_, _) in enumerate(self.data_loader):
                if iter == self.data_loader.dataset.__len__() // self.batch_size:
                    break
  
                z_ = torch.rand((self.batch_size, self.z_dim))
                if self.gpu_mode:
                    x_, z_ = x_.cuda(), z_.cuda()
  
                # update D network
                self.D_optimizer.zero_grad()
  
                D_real, _ = self.D(x_)
                D_real_loss = self.MSE_loss(D_real, x_)
  
                G_ = self.G(z_)
                D_fake, _ = self.D(G_)
                D_fake_loss = self.MSE_loss(D_fake, G_.detach())
  
                D_loss = D_real_loss + torch.clamp(self.margin - D_fake_loss, min=0)
                self.train_hist['D_loss'].append(D_loss.item())
  
                D_loss.backward()
                self.D_optimizer.step()
  
                # update G network
                self.G_optimizer.zero_grad()

                G_ = self.G(z_)
                D_fake, D_fake_code = self.D(G_)
                D_fake_loss = self.MSE_loss(D_fake, G_.detach())
                G_loss = D_fake_loss + self.pt_loss_weight * self.pullaway_loss(D_fake_code.view(self.batch_size, -1))
                self.train_hist['G_loss'].append(G_loss.item())
  
                G_loss.backward()
                self.G_optimizer.step()
  
                if ((iter + 1) % 100) == 0:
                    print("Epoch: [%2d] [%4d/%4d] D_loss: %.8f, G_loss: %.8f" %
                          ((epoch + 1), (iter + 1), self.data_loader.dataset.__len__() // self.batch_size, D_loss.item(), G_loss.item()))
  
            self.train_hist['per_epoch_time'].append(time.time() - epoch_start_time)
            with torch.no_grad():
                self.visualize_results((epoch+1))
  
        self.train_hist['total_time'].append(time.time() - start_time)
        print("Avg one epoch time: %.2f, total %d epochs time: %.2f" % (np.mean(self.train_hist['per_epoch_time']),
              self.epoch, self.train_hist['total_time'][0]))
  
    def pullaway_loss(self, embeddings):
        """ pullaway_loss tensorflow version code
  
            norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
            normalized_embeddings = embeddings / norm
            similarity = tf.matmul(
                normalized_embeddings, normalized_embeddings, transpose_b=True)
            batch_size = tf.cast(tf.shape(embeddings)[0], tf.float32)
            pt_loss = (tf.reduce_sum(similarity) - batch_size) / (batch_size * (batch_size - 1))
            return pt_loss
  
        """
        # norm = torch.sqrt(torch.sum(embeddings ** 2, 1, keepdim=True))
        # normalized_embeddings = embeddings / norm
        # similarity = torch.matmul(normalized_embeddings, normalized_embeddings.transpose(1, 0))
        # batch_size = embeddings.size()[0]
        # pt_loss = (torch.sum(similarity) - batch_size) / (batch_size * (batch_size - 1))
  
        norm = torch.norm(embeddings, 1)
        normalized_embeddings = embeddings / norm
        similarity = torch.matmul(normalized_embeddings, normalized_embeddings.transpose(1, 0)) ** 2
        batch_size = embeddings.size()[0]
        pt_loss = (torch.sum(similarity) - batch_size) / (batch_size * (batch_size - 1))
  
        return pt_loss
  
  
    def visualize_results(self, epoch, fix=True):
        gen_data_plot = self.G(self.sample_z_).cpu().detach()
        fig = plt.figure(figsize = (10,7))
        fig.suptitle(f"Generated Images After {epoch} Epochs", fontsize=16)
        for i in range(16):
            ax = plt.subplot(4, 4, i + 1)
            plt.imshow(gen_data_plot[i].reshape(28, 28), cmap="gray_r")
            plt.xticks([])
            plt.yticks([])
        plt.show()
```

```python
gan = EBGAN()
gan.train()
```

Github에 있는 코드를 실행한 경우 아래와 같이 결과가 잘 나왔다. 50번째 Epoch의 결과이다.

![[team-blog-코딩황제들-2024-week6.세준1.png]]

저번에 내가 짠 코드와 어떤 부분이 다른지 비교해본 결과 network를 업데이트 하는 부분에서는 차이가 거의 없었다. Generator의 Loss를 구할 때 PT를 계산하여 과적합을 막는 부분에서 차이가 있었지만 이 부분은 제거하고 테스트 해 본 결과 이 부분 때문에 문제가 생긴 것 같지는 않았다. 모델을 설계하는 부분에서 차이가 있었다. 이 부분을 좀 더 공부해 봐야 할 것 같다.

**ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ------------**
## 우석
### 목표: 
EBGAN을 이해하고 코드를 돌려본다.

### 결과:
EBGAN의 loss를 구하는 식을 알게 되었다.
#### 판별자의 손실 함수

${L}_D^{EBGAN}​={D}_{AE}​(x)+\max (0,m−{D}_{AE}​(G(z)))$

**DAE​(x)**: 입력 데이터 x에 대해 Autoencoder 기반 판별자가 계산한 에너지 또는 손실이다.

**DAE​(G(z))**: 생성된 데이터 G(z)에 대해 Autoencoder 기반 판별자가 계산한 에너지 또는 손실이다.

**m**: 마진(margin) 값으로, 에너지 기반 판별자의 판단을 더 명확하게 만들어주는 임계값이다. 판별자가 원본 데이터와 생성된 데이터의 에너지를 구분하는 데 사용된다.

**max(0,m−DAE​(G(z)))**: 생성된 데이터의 에너지가 마진 m보다 낮을 때만 손실에 기여하게 하며, 이를 통해 판별자가 생성된 데이터를 구분하도록 유도한다.

​
#### 생성자의 손실 함수

${L}_G^{EBGAN}​={D}_{AE}​(G(z))+λ⋅PT$

**DAE​(G(z))**: 생성자가 생성한 데이터 G(z)에 대한 판별자의 에너지이다. 생성자는 이 에너지를 최소화하려고 시도한다.

**λ⋅PT**: 여기서 PT는 Pulling Term으로, 생성된 데이터 분포가 실제 데이터 분포와 유사하게 만들기 위한 추가적인 제약이나 규제이다.

λ는 이 항목의 중요성을 조정하는 하이퍼파라미터이다.

​
**[출처]** [EBM (8/18)](https://blog.naver.com/sws040201/223553540153)|**작성자** [sws040201](https://blog.naver.com/sws040201)

**ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ------------**
## 채연
### 목표: 
대조학습코드를 구현해본다.

### 결과:
대조 학습에서 주로 사용되는 방식 중 긍정(positive) 쌍과 부정(negative) 쌍을 구분하여 손실을 계산 ‘’’ import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim

#### 간단한 대조 학습 모델 정의

class SimpleContrastiveModel(nn.Module): def **init**(self, embedding_dim=128): super(SimpleContrastiveModel, self).**init**() # CNN 기반 인코더 정의: 입력 이미지를 임베딩 벡터로 변환 self.encoder = nn.Sequential( nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1), # 1번째 레이어 (3 -> 64) nn.ReLU(), # 활성화 함수 ReLU nn.MaxPool2d(kernel_size=2, stride=2), # 풀링 레이어 nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), # 2번째 레이어 (64 -> 128) nn.ReLU(), # 활성화 함수 ReLU nn.MaxPool2d(kernel_size=2, stride=2), # 풀링 레이어 nn.Flatten(), # 출력 데이터를 1차원 벡터로 변환 nn.Linear(128 * 8 * 8, embedding_dim) # 완전 연결 레이어, 임베딩 차원으로 변환 )

```
def forward(self, x):
    # 입력 이미지 x를 임베딩 벡터로 변환하고 정규화
    return F.normalize(self.encoder(x), dim=-1)
```

#### 대조 학습 손실 함수 정의

def contrastive_loss(out1, out2, label): # 코사인 유사도 계산: 두 임베딩 벡터 간의 유사도 측정 cosine_similarity = F.cosine_similarity(out1, out2) # BCEWithLogitsLoss를 사용하여 대조 학습 손실 계산 bce_loss = nn.BCEWithLogitsLoss() # 코사인 유사도와 실제 lable을 사용하여 손실 계산 loss = bce_loss(cosine_similarity, label) return lossine_similarity, label) return loss

#### 모델 초기화

model = SimpleContrastiveModel(embedding_dim=128)

#### Adam 옵티마이저 설정

optimizer = optim.Adam(model.parameters(), lr=0.001)

#### 예제 데이터 생성 (batch size: 16, channel: 3, height: 32, width: 32)

batch_size = 16 x1 = torch.randn(batch_size, 3, 32, 32) x2 = torch.randn(batch_size, 3, 32, 32)

labels = torch.randint(0, 2, (batch_size,)).float() # 1: 같은 클래스, 0: 다른 클래스

#### 모델 출력 계산

out1 = model(x1) out2 = model(x2)

#### 대조 학습 손실 계산

loss = contrastive_loss(out1, out2, labels)

#### 역전파 및 파라미터 업데이트

optimizer.zero_grad() loss.backward() optimizer.step()

print(f’Loss: {loss.item()}’) ‘’’ ————————————————————————————————– ‘’’ import torch import torch.nn as nn import torch.optim as optim

#### SimpleContrastiveModel 클래스와 contrastive_loss 함수가 정의되어 있다고 가정합니다.

#### 모델 초기화

model = SimpleContrastiveModel(embedding_dim=128)

#### 옵티마이저 설정

optimizer = optim.Adam(model.parameters(), lr=0.001)

#### 훈련 데이터 생성 (배치 크기: 16, 채널: 3, 높이: 32, 너비: 32)

batch_size = 16 x1_train = torch.randn(batch_size, 3, 32, 32) x2_train = torch.randn(batch_size, 3, 32, 32) train_labels = torch.randint(0, 2, (batch_size,)).float()

#### 테스트 데이터 생성 (배치 크기: 8, 채널: 3, 높이: 32, 너비: 32)

x1_test = torch.randn(8, 3, 32, 32) x2_test = torch.randn(8, 3, 32, 32) test_labels = torch.randint(0, 2, (8,)).float()

#### 모델 훈련

num_epochs = 10 for epoch in range(num_epochs): # 모델 출력 계산 out1_train = model(x1_train) out2_train = model(x2_train)

```
# 대조 학습 손실 계산
loss = contrastive_loss(out1_train, out2_train, train_labels)

# 역전파 및 파라미터 업데이트
optimizer.zero_grad()
loss.backward()
optimizer.step()

print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')
```

#### 모델 테스트

with torch.no_grad(): out1_test = model(x1_test) out2_test = model(x2_test)

```
# 코사인 유사도 계산
cosine_similarity_test = torch.sigmoid(F.cosine_similarity(out1_test, out2_test))

# 예측 결과
predictions = (cosine_similarity_test > 0.5).float()

# 정확도 계산
accuracy = (predictions == test_labels).float().mean()

print(f'Test Accuracy: {accuracy.item() * 100:.2f}%')

# 각 테스트 샘플에 대한 코사인 유사도와 실제 레이블 출력
print("Cosine Similarity and Labels:")
for i in range(len(test_labels)):
    print(f"Sample {i+1}: Cosine Similarity = {cosine_similarity_test[i].item():.4f}, Label = {test_labels[i].item()}, Prediction = {predictions[i].item()}")
```

’’’ Epoch [1/10], Loss: 0.8842123746871948 Epoch [2/10], Loss: 0.9274642467498779 Epoch [3/10], Loss: 0.9250276684761047 Epoch [4/10], Loss: 0.9145865440368652 Epoch [5/10], Loss: 0.866182267665863 Epoch [6/10], Loss: 0.5781313180923462 Epoch [7/10], Loss: 0.557551383972168 Epoch [8/10], Loss: 0.5832924842834473 Epoch [9/10], Loss: 0.41409537196159363 Epoch [10/10], Loss: 0.4382629096508026 Test Accuracy: 12.50% Cosine Similarity and Labels: Sample 1: Cosine Similarity = 0.7111, Label = 0.0, Prediction = 1.0 Sample 2: Cosine Similarity = 0.7042, Label = 0.0, Prediction = 1.0 Sample 3: Cosine Similarity = 0.7109, Label = 0.0, Prediction = 1.0 Sample 4: Cosine Similarity = 0.7049, Label = 0.0, Prediction = 1.0 Sample 5: Cosine Similarity = 0.7111, Label = 1.0, Prediction = 1.0 Sample 6: Cosine Similarity = 0.7117, Label = 0.0, Prediction = 1.0 Sample 7: Cosine Similarity = 0.7063, Label = 0.0, Prediction = 1.0 Sample 8: Cosine Similarity = 0.7098, Label = 0.0, Prediction = 1.0

문제점 **손실 값 감소** 훈련 중 손실 값이 감소하는 것은 모델이 훈련 데이터에 대해 학습하고 있음을 나타낸다. 그러나 손실 값의 감소가 매우 불규칙하고, 손실 값이 매우 높고 후반부에 급격히 감소하는 모양이다. 이는 모델이 안정적으로 학습되지 않았거나 데이터의 질과 양에 문제가 있을 수 있음을 의미한다.

**테스트 정확도** 테스트 정확도가 매우 낮다. 이는 모델이 테스트 데이터에 대해 거의 무작위로 예측하고 있음을 의미한다.

**코사인 유사도 및 예측** 코사인 유사도 값이 모든 샘플에서 매우 유사하게 높게 나타나고 있으며, 이는 모델이 제대로 된 임베딩을 학습하지 못하고 있다는 이야기다. 모든 샘플에 대해 거의 동일한 값을 출력하고 있으며, 이로 인해 예측이 실제 레이블과 일치하지 않는 상황이 발생한다.

해결 방안 **데이터셋 품질 개선** 가상 데이터셋 대신 실제 데이터셋으로 모델을 훈련 가상 데이터는 모델 학습에 필요한 다양성과 복잡성이 부족할 수 있다.

**모델 구조 수정** 모델이 너무 단순하여 데이터의 복잡한 패턴을 학습하지 못할 수 있다. 더 깊고 복잡한 모델을 사용하거나, 현재 아키텍처에 더 많은 층을 추가하여 모델의 구조를 변경할 수 있다.

**학습 하이퍼파라미터 조정** 학습률을 조정하여 모델의 학습 속도를 최적화할 수 있다. 에포크 수를 늘려 더 오랜 시간 동안 학습하게 하여 성능을 개선 배치사이즈를 조정하여 학습 안정성을 도모 커널 사이즈를 조정하여 모델의 복잡성 및 특징에 대하여 학습

**손실 함수 및 학습 방식 검토** BCEWithLogitsLoss를 사용하는 방법이 적절한지 확인해야 한다. 공부한 내용이라 사용하긴 하였지만, 적절한 방법인지에 대한 고려 없이 사용하여 잘못된 결과가 도출되었을 수도 있다. 코사인 유사도가 매우 좁은 범위에 몰려 있다면, 모델이 효과적으로 학습하지 못할 수 있다. 이를 위해 대조 학습 손실의 다른 변형을 사용하거나, 추가적인 정규화 기법을 도입할 수 있다.

**ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ------------**
## 인증샷

![[team-blog-코딩황제들-2024-week6.모각코6일차회의인증.png]]
![[team-blog-코딩황제들-2024-week6.모각코6일차시간인증.jpg]]


