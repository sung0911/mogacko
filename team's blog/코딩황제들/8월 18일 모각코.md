# 계획
energy-based model, 특히 energy-based GAN에 대해 알아보기 및 대조학습코드 구현 


**회의방법**
온라인(naver whale on) *zoom은 40분이상하려면 유료로 결제를 해야하기 때문에 whale on을 활용했습니다. 

**팀원 블로그**
박세준 https://kepler-dev-3141.github.io/
신우석 https://blog.naver.com/sws040201/
김채연 https://kcyeon0127.github.io/

# 결과
## 준혁:
### 목표: 
에너지 기반 GAN(Energy-Based Generative Adversarial Network, EBGAN)에 대해 알아본다.


### 결과:

#### 1. EBGAN이란?
*전통적인 GAN모델의 변형으로, 에너지 기반 모델(EBM)의 개념을 도입하여 GAN의 판별자(Discriminator)를 재설계한 모델입니다. EBGAN은 전통적인 GAN보다 더 안정적이고 유연한 학습을 가능하게 하기 위해 제안되었습니다.

#### 2. **전통적인 GAN의 기본 개념**

- **생성자(Generator):** 랜덤한 노이즈 벡터를 입력으로 받아, 이로부터 현실적인 데이터를 생성하는 네트워크입니다.
- **판별자(Discriminator):** 입력 데이터가 실제(real) 데이터인지 생성된(fake) 데이터인지 구분하는 네트워크입니다.
- GAN의 목표는 생성자가 점점 더 현실적인 데이터를 생성하도록 학습하는 것입니다. 생성자가 만든 데이터가 실제 데이터를 닮아가면서, 판별자는 점점 더 어려운 구별 문제를 해결해야 하므로 두 네트워크가 서로 경쟁하면서 발전합니다.

#### 3. **EBGAN에서의 변화**

- **에너지 기반 판별자:** EBGAN에서 판별자는 단순히 이진 분류를 수행하는 대신, 데이터를 입력받아 그 데이터의 "에너지"를 계산합니다. 이 에너지는 데이터가 얼마나 "현실적인지"를 나타내며, EBM에서 사용되는 에너지 함수와 유사한 역할을 합니다.
- **목표:** EBGAN의 목표는 생성된 데이터에 높은 에너지를, 실제 데이터에 낮은 에너지를 할당하는 것입니다.

#### 4. **EBGAN의 작동 방식**

- **판별자의 역할:** EBGAN에서 판별자는 입력 데이터 xxx에 대해 에너지 값 E(x)E(x)E(x)를 계산합니다. 이 에너지는 데이터가 실제에 가까울수록 낮게, 비현실적일수록 높게 설정됩니다.
- **생성자의 역할:** 생성자는 낮은 에너지를 가진 데이터를 생성하도록 학습합니다. 즉, 판별자가 낮은 에너지를 할당하는 데이터를 생성하려고 시도합니다.
- **손실 함수:** EBGAN의 손실 함수는 전통적인 GAN과 다르게, 판별자가 계산한 에너지 값을 최소화하거나 최대화하는 방식으로 정의됩니다.

#### 5. **EBGAN의 장점**

- **학습의 안정성:** 전통적인 GAN에서는 종종 학습이 불안정해지거나 모드 붕괴(mode collapse)가 발생할 수 있습니다. EBGAN은 에너지 기반 접근 방식을 통해 이러한 문제를 완화할 수 있습니다.
- **유연성:** 판별자가 에너지를 계산하는 방식은 단순한 이진 분류보다 더 유연하여, 다양한 형태의 손실 함수나 학습 전략을 적용할 수 있습니다.

#### 6. **예시**

- 예를 들어, 이미지 생성 문제에서 EBGAN은 판별자가 이미지에 대해 "에너지"를 계산하고, 생성자는 이 에너지를 낮추는 방향으로 이미지를 생성합니다. 결과적으로, 생성된 이미지는 더 현실적이고, 판별자는 더 까다로운 기준으로 이미지를 평가하게 됩니다.

#### 7. **EBGAN의 응용**

- **이미지 생성:** 현실적인 이미지를 생성하는 데 사용할 수 있습니다. 특히, 고해상도 이미지 생성이나 다양한 스타일의 이미지 생성에 효과적일 수 있습니다.
- **비디오 생성:** 비디오의 연속적인 프레임을 생성할 때, 각 프레임이 이전 프레임과의 연속성을 가지면서도 개별적으로 현실적인 에너지를 가지도록 학습할 수 있습니다.
- **자연어 처리:** 텍스트 생성에서도 EBGAN을 응용하여 더 자연스러운 문장을 생성하는 데 사용될 수 있습니다.


## 세준:
### 목표: 


### 결과:

## 우석:
### 목표: 


### 결과:


## 채연:
### 목표: 
대조학습코드를 구현해본다.


### 결과:
대조 학습에서 주로 사용되는 방식 중 긍정(positive) 쌍과 부정(negative) 쌍을 구분하여 손실을 계산 ‘’’ import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim

#### 간단한 대조 학습 모델 정의

class SimpleContrastiveModel(nn.Module): def **init**(self, embedding_dim=128): super(SimpleContrastiveModel, self).**init**() # CNN 기반 인코더 정의: 입력 이미지를 임베딩 벡터로 변환 self.encoder = nn.Sequential( nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1), # 1번째 레이어 (3 -> 64) nn.ReLU(), # 활성화 함수 ReLU nn.MaxPool2d(kernel_size=2, stride=2), # 풀링 레이어 nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), # 2번째 레이어 (64 -> 128) nn.ReLU(), # 활성화 함수 ReLU nn.MaxPool2d(kernel_size=2, stride=2), # 풀링 레이어 nn.Flatten(), # 출력 데이터를 1차원 벡터로 변환 nn.Linear(128 * 8 * 8, embedding_dim) # 완전 연결 레이어, 임베딩 차원으로 변환 )

```
def forward(self, x):
    # 입력 이미지 x를 임베딩 벡터로 변환하고 정규화
    return F.normalize(self.encoder(x), dim=-1)
```

#### 대조 학습 손실 함수 정의

def contrastive_loss(out1, out2, label): # 코사인 유사도 계산: 두 임베딩 벡터 간의 유사도 측정 cosine_similarity = F.cosine_similarity(out1, out2) # BCEWithLogitsLoss를 사용하여 대조 학습 손실 계산 bce_loss = nn.BCEWithLogitsLoss() # 코사인 유사도와 실제 lable을 사용하여 손실 계산 loss = bce_loss(cosine_similarity, label) return lossine_similarity, label) return loss

#### 모델 초기화

model = SimpleContrastiveModel(embedding_dim=128)

#### Adam 옵티마이저 설정

optimizer = optim.Adam(model.parameters(), lr=0.001)

#### 예제 데이터 생성 (batch size: 16, channel: 3, height: 32, width: 32)

batch_size = 16 x1 = torch.randn(batch_size, 3, 32, 32) x2 = torch.randn(batch_size, 3, 32, 32)

labels = torch.randint(0, 2, (batch_size,)).float() # 1: 같은 클래스, 0: 다른 클래스

#### 모델 출력 계산

out1 = model(x1) out2 = model(x2)

#### 대조 학습 손실 계산

loss = contrastive_loss(out1, out2, labels)

#### 역전파 및 파라미터 업데이트

optimizer.zero_grad() loss.backward() optimizer.step()

print(f’Loss: {loss.item()}’) ‘’’ ————————————————————————————————– ‘’’ import torch import torch.nn as nn import torch.optim as optim

#### SimpleContrastiveModel 클래스와 contrastive_loss 함수가 정의되어 있다고 가정합니다.

#### 모델 초기화

model = SimpleContrastiveModel(embedding_dim=128)

#### 옵티마이저 설정

optimizer = optim.Adam(model.parameters(), lr=0.001)

#### 훈련 데이터 생성 (배치 크기: 16, 채널: 3, 높이: 32, 너비: 32)

batch_size = 16 x1_train = torch.randn(batch_size, 3, 32, 32) x2_train = torch.randn(batch_size, 3, 32, 32) train_labels = torch.randint(0, 2, (batch_size,)).float()

#### 테스트 데이터 생성 (배치 크기: 8, 채널: 3, 높이: 32, 너비: 32)

x1_test = torch.randn(8, 3, 32, 32) x2_test = torch.randn(8, 3, 32, 32) test_labels = torch.randint(0, 2, (8,)).float()

#### 모델 훈련

num_epochs = 10 for epoch in range(num_epochs): # 모델 출력 계산 out1_train = model(x1_train) out2_train = model(x2_train)

```
# 대조 학습 손실 계산
loss = contrastive_loss(out1_train, out2_train, train_labels)

# 역전파 및 파라미터 업데이트
optimizer.zero_grad()
loss.backward()
optimizer.step()

print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')
```

#### 모델 테스트

with torch.no_grad(): out1_test = model(x1_test) out2_test = model(x2_test)

```
# 코사인 유사도 계산
cosine_similarity_test = torch.sigmoid(F.cosine_similarity(out1_test, out2_test))

# 예측 결과
predictions = (cosine_similarity_test > 0.5).float()

# 정확도 계산
accuracy = (predictions == test_labels).float().mean()

print(f'Test Accuracy: {accuracy.item() * 100:.2f}%')

# 각 테스트 샘플에 대한 코사인 유사도와 실제 레이블 출력
print("Cosine Similarity and Labels:")
for i in range(len(test_labels)):
    print(f"Sample {i+1}: Cosine Similarity = {cosine_similarity_test[i].item():.4f}, Label = {test_labels[i].item()}, Prediction = {predictions[i].item()}")
```

’’’ Epoch [1/10], Loss: 0.8842123746871948 Epoch [2/10], Loss: 0.9274642467498779 Epoch [3/10], Loss: 0.9250276684761047 Epoch [4/10], Loss: 0.9145865440368652 Epoch [5/10], Loss: 0.866182267665863 Epoch [6/10], Loss: 0.5781313180923462 Epoch [7/10], Loss: 0.557551383972168 Epoch [8/10], Loss: 0.5832924842834473 Epoch [9/10], Loss: 0.41409537196159363 Epoch [10/10], Loss: 0.4382629096508026 Test Accuracy: 12.50% Cosine Similarity and Labels: Sample 1: Cosine Similarity = 0.7111, Label = 0.0, Prediction = 1.0 Sample 2: Cosine Similarity = 0.7042, Label = 0.0, Prediction = 1.0 Sample 3: Cosine Similarity = 0.7109, Label = 0.0, Prediction = 1.0 Sample 4: Cosine Similarity = 0.7049, Label = 0.0, Prediction = 1.0 Sample 5: Cosine Similarity = 0.7111, Label = 1.0, Prediction = 1.0 Sample 6: Cosine Similarity = 0.7117, Label = 0.0, Prediction = 1.0 Sample 7: Cosine Similarity = 0.7063, Label = 0.0, Prediction = 1.0 Sample 8: Cosine Similarity = 0.7098, Label = 0.0, Prediction = 1.0

문제점 **손실 값 감소** 훈련 중 손실 값이 감소하는 것은 모델이 훈련 데이터에 대해 학습하고 있음을 나타낸다. 그러나 손실 값의 감소가 매우 불규칙하고, 손실 값이 매우 높고 후반부에 급격히 감소하는 모양이다. 이는 모델이 안정적으로 학습되지 않았거나 데이터의 질과 양에 문제가 있을 수 있음을 의미한다.

**테스트 정확도** 테스트 정확도가 매우 낮다. 이는 모델이 테스트 데이터에 대해 거의 무작위로 예측하고 있음을 의미한다.

**코사인 유사도 및 예측** 코사인 유사도 값이 모든 샘플에서 매우 유사하게 높게 나타나고 있으며, 이는 모델이 제대로 된 임베딩을 학습하지 못하고 있다는 이야기다. 모든 샘플에 대해 거의 동일한 값을 출력하고 있으며, 이로 인해 예측이 실제 레이블과 일치하지 않는 상황이 발생한다.

해결 방안 **데이터셋 품질 개선** 가상 데이터셋 대신 실제 데이터셋으로 모델을 훈련 가상 데이터는 모델 학습에 필요한 다양성과 복잡성이 부족할 수 있다.

**모델 구조 수정** 모델이 너무 단순하여 데이터의 복잡한 패턴을 학습하지 못할 수 있다. 더 깊고 복잡한 모델을 사용하거나, 현재 아키텍처에 더 많은 층을 추가하여 모델의 구조를 변경할 수 있다.

**학습 하이퍼파라미터 조정** 학습률을 조정하여 모델의 학습 속도를 최적화할 수 있다. 에포크 수를 늘려 더 오랜 시간 동안 학습하게 하여 성능을 개선 배치사이즈를 조정하여 학습 안정성을 도모 커널 사이즈를 조정하여 모델의 복잡성 및 특징에 대하여 학습

**손실 함수 및 학습 방식 검토** BCEWithLogitsLoss를 사용하는 방법이 적절한지 확인해야 한다. 공부한 내용이라 사용하긴 하였지만, 적절한 방법인지에 대한 고려 없이 사용하여 잘못된 결과가 도출되었을 수도 있다. 코사인 유사도가 매우 좁은 범위에 몰려 있다면, 모델이 효과적으로 학습하지 못할 수 있다. 이를 위해 대조 학습 손실의 다른 변형을 사용하거나, 추가적인 정규화 기법을 도입할 수 있다.

