# 계획
프로젝트의 목표에 맞는 데이터 특성에 가장 적합한 손실 함수를 찾기 위해 다양한 손실함수에 대해 공부한다. 각 로스함수의 수학적 정의 및 유도과정 이해, 각 로스함수가 사용되는 모델과 그 이유(어떤 모델과 문제에 적합한지), 장 단점 분석, 간단한 구현예제를 작성해보면서 이해해본다. 또한 다양한 학습방법 및 디퓨전모델에 대해서도 알아본다.

**회의방법**
온라인(naver whale on) *zoom은 40분이상하려면 유료로 결제를 해야하기 때문에 whale on을 활용했습니다.

**팀원 블로그**
박세준 https://kepler-dev-3141.github.io/
신우석 https://blog.naver.com/sws040201/
김채연 https://kcyeon0127.github.io/


# 결과

## 준혁: 
### 목표:
 Earth Mover's Distance (EMD) 개념을 바탕으로 wassertein distance 및 wassertein 손실함수에 대한 개념을 확실히 정립한다.
 
### 결과:
#### **1. wassertain distance란?:**
러시아 수학자 Leanid Vaserstein 의 이름을 딴 것으로 Roland Dobtushin 교수가 1970년에 확률론에 도입한 것이다.
![[team-blog-코딩황제들-2024-week2.wasserstain공식.png]]
GAN에서 discriminator가 학습도중에 잘 죽는 현상이 나타나는데, 이를 해결하고자 하는것이 wasserstain loss이다. 이는 wasserstein distance를 최소화시키는 것이 목표이다.
위식에서 위의 내용이 Wasserstein distance의 정의다.

여기에서 ![](https://t1.daumcdn.net/cfile/tistory/9984A03359A4215B2E) 는 두 확률분포 , P,Q의 결합확률분포(joint distribution)들을 모은 집합이고, 그 중에 

![](https://t1.daumcdn.net/cfile/tistory/991E073359A4217412)는 그 중 하나입니다. 즉 모든 결합확률분포 ![](https://t1.daumcdn.net/cfile/tistory/9951703359A4218E22) 중에서 d(x,y)의 기댓값을 가장 작게 추정한 값을 의미합니다. 

즉 두 확률분포의 연관성을 측정하여 그 거리의 기대값이 가장 작을때의 distance를 wasserstein distance라고 얘기를 합니다. 

#### 1-2.  wassertain distance 쉽게 이해

**주변 확률 분포 P와 Q**:

- P와 Q는 각각 X와 Y의 주변 확률 분포입니다.
- 주변 확률 분포는 각각의 확률 변수가 따르는 분포의 모양을 나타냅니다.
- 예를 들어, P와 Q가 모두 정규 분포라면, X와 Y는 각각 정규 분포를 따릅니다.
**결합 확률 분포 γ**:

- γ는 X와 Y의 결합 확률 분포로, w에 따라 샘플링된 X와 Y의 쌍의 분포를 나타냅니다.
- 결합 확률 분포는 두 확률 변수 간의 관계(의존성)를 나타냅니다.
- 다양한 γ는 서로 다른 X와 Y의 결합 방식을 의미합니다.
##### 1-2-1.
![[team-blog-코딩황제들-2024-week2.wasserstain설명1.png]]

표본 공간에서 w를 하나 샘플링 하면 X(w)와 Y(w)를 뽑을 수 있고 이때 두 점 간의 거리 d(X(w),Y(w)) 역시 계산 할수 있다.
##### 1-2-2.
![[team-blog-코딩황제들-2024-week2.wasserstain설명2.png]]

샘플링을 계속 할수록 (X, Y)의 결합 확률 분포 γ의 윤곽이 나오게 더불어서 (P, Q)는 γ의 주변확률분포가 됩니다.
##### 1-2-3.
![[team-blog-코딩황제들-2024-week2.wasserstain설명3.png]]
이때 γ가 두 확률변수 X, Y의 연관성을 어떻게 측정하느냐에 따라 d(X, Y)의 분포가 달라지게 됩니다.
##### 1-2-4.
![[team-blog-코딩황제들-2024-week2.wasserstain설명4.png]]
주의할 점은 P와 Q는 바뀌지 않기 때문에 각 X와 Y 가 분포하는 모양은 변하지 않습니다. 다만 w에 따라 뽑히는 경향이 달라질 뿐이다.

###### 예시 1: 정규 분포와 균등 분포

- **P와 Q**:
    
    - X는 정규 분포 P를 따릅니다.
    - Y는 균등 분포 Q를 따릅니다.
- **결합 확률 분포 γ1​**:
    
    - γ1​에서는 X가 클 때 Y도 클 확률이 높습니다.
    - 즉, ω가 큰 값을 가질 때 X(ω)와 Y(ω) 모두 큰 값을 가질 경향이 있습니다.
- **결합 확률 분포 γ2​**:
    
    - γ2​에서는 X가 클 때 Y가 작을 확률이 높습니다.
    - 즉, ω가 큰 값을 가질 때 X(ω)는 큰 값을, Y(ω)는 작은 값을 가질 경향이 있습니다.
**요약**

- **변하지 않는 것**:
    
    - P와 Q의 모양(즉, X와 Y의 분포)은 변하지 않습니다.
    - X는 항상 정규 분포를 따르고, Y는 항상 균등 분포를 따릅니다.
- **변하는 것**:
    
    - γ에 따라 X와 Y가 뽑히는 경향이 달라집니다.
    - 즉, 두 변수 X와 Y의 결합 관계(의존성)가 달라집니다.
##### 1-2-5.
![[team-blog-코딩황제들-2024-week2.wasserstain설명5.png]]
Wasserstein distance 는 이렇게 여러가지 γ중에서 d(X, Y) 의 기댓값이 가장 작게 나오는 확률분포를 취하게 된다.

그래서 Wasserstein GAN은 이 Wasserstein distance를 이용해서 GAN의 문제를 푸는 것이다.

### 출처: [https://dogfoottech.tistory.com/185](https://dogfoottech.tistory.com/185) [Magritte 기술블로그:티스토리]

## 세준: 
### 목표

Diffusion Model의 간단한 정의와 학습 방법에 대해 알아보기

### 결과

#### 1. 정의

Diffusion Model이란 입력 이미지에 여러 단계로 노이즈를 가하고, 거꾸로 노이즈를 없애는 방향으로 학습을 하여 입력 이미지와 비슷한 이미지를 생성하는 모델입니다. Diffusion Model은 기존 모델이 가지고 있는 tractability와 flexibility의 trade-off 관계를 해결하기 위해 만들어졌습니다. tractable하다는 것은 분석적으로 평가할 수 있고 잘 설명할 수 있다는 것 이고 flexible하다는 것은 많은 데이터에 대해 설명할 수 있다는 것을 말합니다.

논문 저자들이 제안한 새로운 확률 모델의 특징은 아래와 같습니다.

1. 모델 구조의 극단적인 유연성
2. 정확한 샘플링
3. 다른 분포와의 쉬운 곱셈
4. 모델 로그 가능도와 개별 상태의 확률을 저렴하게 평가할 수 있음

#### 2. 알고리즘

##### 2.1 Forward diffusion

![[team-blog-코딩황제들-2024-week2.세준1.png]]  
Forward Diffusion 과정은 원본 이미지 X0에서 점차 노이즈를 가해 최종적으로는 완전한 노이즈 이미지인 XT가 되게 됩니다.

##### 2.2 Reverse diffusion

![[team-blog-코딩황제들-2024-week2.세준2.png]]  
Forward Diffusion 과정은 노이즈 이미지 XT로부터 노이즈를 점차 제거해 나가 X0가 되게 됩니다.

#### 참고

[https://proceedings.mlr.press/v37/sohl-dickstein15.html](https://proceedings.mlr.press/v37/sohl-dickstein15.html)  
[https://aigong.tistory.com/569](https://aigong.tistory.com/569)  
[https://ffighting.net/deep-learning-paper-review/diffusion-model/diffusion-model-basic/](https://ffighting.net/deep-learning-paper-review/diffusion-model/diffusion-model-basic/)


## 우석:

### 목표 :

sparse_categorical cross entropy (CCE)에 대해 알아본다.

### 결과 :

Sparse_Categorical Cross Entropy : 라벨이 정수 형태로 주어지는 CCE이다.
ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ

​라벨이 정수 형태인 경우 Sparse_Categorical Cross Entropy를 사용한다. ex) [0, 1, 2, ... ]

$L=-\frac{1}{N}\sum _{j=1}^N\log \left({q}_{{yj}}\right)$

yj는\ 훈련\ 샘플의\ 라벨,\ \ qyj는\ \ 예측한\ \ 클래스\ \ yj에\ \ 대한\ \ 확률


​

**모델 학습 과정**

​입력 데이터가 모델을 통해 전달되어 각 클래스에 대한 예측 확률을 출력한다. 이때 주로 softmax 활성화 함수를 사용하여 예측 확률을 계산한다.

CCE 손실 함수를 사용하여 예측 확률과 실제 레이블 간의 차이를 측정한다.

손실 함수의 미분값(그래디언트)을 계산하여 모델의 가중치를 업데이트하여 예측을 더 정확하게 만들 수 있다.

**[출처]** [Sparse_Categorical Cross Entropy (7/21)](https://blog.naver.com/sws040201/223520799869)|**작성자** [sws040201](https://blog.naver.com/sws040201)


## 채연:
### 목표:
BCELoss, BCEWithLogitsLoss, CrossEntropyLoss function에 대해 알아본다.

### 결과:

BCELoss 크로스 엔트로피 손실 함수는 정보 이론에서 크로스 엔트로피 개념을 기계 학습에 적용한 것이다. 이 함수는 두 확률 분포 간의 차이를 측정하는 방법으로, BCE 손실 함수는 크로스 엔트로피 손실 함수를 이진 분류 문제에 적용한 형태이다.

[ H(P, Q) = -\sum_{x} P(x) \log Q(x) ]

BCELoss는 모델의 구조 상에 마지막 Layer가 Sigmoid 혹은 Softmax로 되어 있는 경우 이를 사용한다. 즉, 모델의 출력이 각 라벨에 대한 확률값으로 구성되었을 때 사용이 가능하다. ''' torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean') '''

''' import torch import torch.nn as nn m = nn.Sigmoid() loss = nn.BCELoss() input = torch.randn(3, 2, requires_grad=True) target = torch.rand(3, 2) output = loss(m(input), target) output.backward() '''

BCEWithLogitsLoss ''' torch.nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None) '''

BCEWithLogitsLoss는 이름에서도 유추해볼 수 있듯 BCELoss를 위 과정에서 확률값(Logits)으로 변환하지 않더라도 계산되는 것을 의미한다. 기본적인 BCE 손실 함수는 모델의 출력이 시그모이드 함수를 통과한 확률 값이어야 한다. 그러나 이 경우수치적 불안정성(시그모이드 함수의 출력은 0과 1 사이의 값이기 때문에, 극단적인 값(예: 매우 큰 음수나 양수)에 대해 수치적으로 불안정할 수 있다.) 효율성 문제(시그모이드 함수와 BCE 손실 함수를 따로 적용하면 계산 비용이 증가할 수 있다.)가 존재할 수 있다

따라서 BCEWithLogitsLoss는 시그모이드 활성화 함수를 적용한 후 BCE 손실을 계산하는 과정을 하나의 함수로 처리한다. 내부적으로 시그모이드 함수를 적용하고 BCE 손실을 계산하므로, 더 안정적이고 효율적이다.

[ \text{BCEWithLogitsLoss} = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(\sigma(z_i)) + (1 - y_i) \log(1 - \sigma(z_i)) \right] ]

여기서:

- ( N )은 샘플의 수
- ( y_i )는 실제 레이블 (0 또는 1)
- ( z_i )는 모델의 출력(로그 확률)
- ( \sigma(z) )는 시그모이드 함수로, (\sigma(z) = \frac{1}{1 + e^{-z}} )

CrossEntropyLoss ''' torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean', label_smoothing=0.0) ''' weight:

타입: Tensor, 선택 사항 설명: 각 클래스에 대한 가중치를 지정할 수 있습니다. 클래스 불균형 문제를 해결하기 위해 사용됩니다. None으로 설정하면 모든 클래스에 동일한 가중치가 적용됩니다. size_average:

타입: bool, 선택 사항 (기본값: None) 설명: 이 파라미터는 더 이상 사용되지 않습니다. reduction 파라미터로 대체되었습니다. True로 설정하면 손실이 평균화되고, False로 설정하면 합산됩니다. ignore_index:

타입: int, 선택 사항 설명: 특정 클래스 인덱스를 무시할 수 있습니다. 주로 시퀀스 모델링에서 패딩 토큰을 무시하는 데 사용됩니다. 기본값은 -100입니다. reduce:

타입: bool, 선택 사항 (기본값: None) 설명: 이 파라미터도 더 이상 사용되지 않습니다. reduction 파라미터로 대체되었습니다. True로 설정하면 손실이 축소되고, False로 설정하면 축소되지 않습니다. reduction:

타입: str, 선택 사항 설명: 손실 결과를 어떻게 축소할지를 정의합니다. 세 가지 옵션이 있습니다: 'none': 손실을 축소하지 않고 각 샘플에 대한 손실을 반환합니다. 'mean': 손실을 평균화합니다. (size_average=True와 동일) 'sum': 손실을 합산합니다. (reduce=False와 동일) label_smoothing:

타입: float, 선택 사항 설명: 라벨 스무딩을 적용합니다. label_smoothing 값을 [0, 1] 사이로 설정하면, 라벨을 약간의 확률로 스무딩합니다. 이는 모델이 과도하게 확신하는 것을 방지하고 일반화 성능을 향상시킬 수 있습니다. 기본값은 0.0입니다.

이전에 다룬 BCELoss와 BCEWithLogitsLoss는 Binary Classification을 위한 손실 함수다. 반면에 CrossEntropyLoss는 다중 분류를 위한 손실 함수다. 예를 들어, 라벨이 5개라고 한다면 입력은 각 라벨에 대한 확률값을 표현하고, 정답 라벨은 라벨 값 혹은 라벨에 대한 확률값으로 표현할 수 있다. 소프트맥스 활성화 함수와 크로스 엔트로피 손실을 결합한 함수로 예측 값과 실제 값 간의 차이를 직관적으로 표현할 수 있지만, 확률이 매우 작은 경우, 로그 함수로 인해 수치적 불안정성이 발생할 수 있고, 클래스가 불균형한 경우 성능이 저하될 수 있습니다. 이 문제를 해결하기 위해 가중치 조정 등을 사용할 수 있다.

[ \text{CrossEntropyLoss} = -\sum_{i=1}^{N} \sum_{c=1}^{C} y_{ic} \log(p_{ic}) ]

여기서:

- ( N )은 샘플의 수
- ( C )는 클래스의 수
- ( y_{ic} )는 실제 레이블의 원-핫 인코딩 (실제 레이블이 ( c ) 클래스일 때 1, 그렇지 않으면 0)
- ( p_{ic} )는 모델이 샘플 ( i )에 대해 클래스 ( c )일 확률로 예측한 값 (소프트맥스 함수의 출력)

## 인증샷
![[team-blog-코딩황제들-2024-week2.모각코2일차회의인증.png]]

![[team-blog-코딩황제들-2024-week2.모각코2일차시간인증.JPG]]
