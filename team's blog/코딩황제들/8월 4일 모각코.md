# 계획
프로젝트의 목표에 맞는 데이터 특성에 가장 적합한 손실 함수를 찾기 위해 다양한 손실함수에 대해 공부한다. 각 로스함수의 수학적 정의 및 유도과정 이해, 각 로스함수가 사용되는 모델과 그 이유(어떤 모델과 문제에 적합한지), 장 단점 분석, 간단한 구현예제를 작성해보면서 이해해본다. 또한 다양한 학습방법 및 디퓨전모델에 대해서도 알아본다. 더불어 energy-based model에대해서 알아본다.


**회의방법**
온라인(naver whale on) *zoom은 40분이상하려면 유료로 결제를 해야하기 때문에 whale on을 활용했습니다. 

**팀원 블로그**
박세준 https://kepler-dev-3141.github.io/
신우석 https://blog.naver.com/sws040201/
김채연 https://kcyeon0127.github.io/


# 결과

## 준혁:
### 목표: 
Wasserstein Loss를 사용한 GAN 예제 코드와 Binary Cross-Entropy (BCE) Loss를 사용한 GAN 예제코드를 작성해보면서 BCE-loss의 기울기 소실 문제와 모드 붕괴 문제의 해결책이 될 수 있는지 직접 알아본다.

### 결과:
[Compare GAN with BCE to with Wasser.ipynb - Colab (google.com)](https://colab.research.google.com/drive/1oQKL3AEJPOhZGUZVm0DrDM30hX68jYgz#scrollTo=BhxXGq-SEsQW)

간단한 1차원데이터(숫자)를 랜덤으로 생성을 하여 진행하였습니다.

#### 1. GAN WITH BCE LOSS

![[team-blog-코딩황제들-2024-week4.BCELOSSlossgraph.png]]
![[team-blog-코딩황제들-2024-week4.BCELOSSgradientgraph.png]]
##### BCE Losses 그래프

- Discriminator Loss: 판별자의 손실은 초기에는 약간의 변동을 보이다가 이후 안정화되는 경향을 보입니다. 하지만 전반적인 감소 추세가 뚜렷하지 않고, 일정한 범위 내에서 변동이 반복됩니다.

- Generator Loss: 생성자의 손실은 초기에는 감소하는 경향을 보이다가 이후 증가하는 경향을 보입니다. 이는 생성자가 학습이 진행될수록 판별자를 속이는 능력이 감소하고 있음을 나타낼 수 있습니다.

##### Gradients over Steps 그래프

- **Gradient Mean**: 그래디언트 평균이 초기에는 음수에서 시작해 점차 증가하는 경향을 보입니다. 이는 기울기 소실 문제는 발생하지 않지만, 그래디언트가 매우 작거나 큰 값을 가질 수 있다는 것을 나타냅니다.


##### 평가

###### 1. **Loss 그래프**:
- 판별자와 생성자의 손실이 모두 안정적으로 감소하지 않는 점이 주목됩니다. 특히 생성자의 손실이 후반부에 증가하는 경향은 기울기 소실 문제로 인해 생성자의 학습이 제대로 이루어지지 않고 있음을 나타낼 수 있습니다.
- 손실이 감소하지 않고 변동이 계속되는 것은 모델이 최적화 과정에서 어려움을 겪고 있음을 의미할 수 있습니다.
###### 2. **Gradient 그래프**:
- 그래디언트의 평균이 증가하는 경향을 보이지만, 초기에는 매우 작은 값을 가지다가 점차 커지는 패턴을 보입니다. 이는 기울기 소실 문제는 발생하지 않지만, 그래디언트가 안정적이지 않다는 것을 나타냅니다.
- 그래디언트의 큰 변동은 모델 학습 과정에서 불안정성을 초래할 수 있습니다.

#### 2. GAN WITH WASSERSTAIN LOSS

![[team-blog-코딩황제들-2024-week4.WASSERSTAINLOSSlossgraph.png]]
![[team-blog-코딩황제들-2024-week4.WASSERSTAINLOSSlossgraph.png]]
** GP란?
*GP는 Gradient Penalty로 모델 학습 중 기울기의 크기를 일정하게 유지하도록 강제하는 방법입니다. 이를 통해 기울기 소실 문제를 완화하고, 생성자와 판별자가 더 안정적으로 학습할 수 있게 합니다. Gradient Penalty는 실제 데이터와 생성된 데이터 사이의 보간(interpolation) 데이터를 사용하여 계산됩니다. 보간 데이터에서 기울기를 계산하고, 이 기울기가 1에 가깝도록 제약을 가합니다.*
##### WGAN-GP Losses 그래프

- **Discriminator Loss**: 전반적으로 감소하는 추세를 보이지만, 중간중간 변동이 있습니다. 이는 훈련 중 판별자가 생성자의 학습에 따라 변동하는 정상적인 현상일 수 있습니다.
- **Generator Loss**: 초기에는 거의 변동이 없지만, 점차 감소하는 추세를 보입니다. 이는 생성자가 판별자를 속이는 능력을 점차 향상시키고 있다는 것을 나타냅니다.

##### Gradients over Steps 그래프

- **Gradient Mean**: 그래디언트 평균이 0에 가까운 값을 유지하다가 중간에 급격히 증가했다가 다시 감소하는 패턴을 보입니다. 이는 기울기 소실 문제를 해결했음을 나타낼 수 있습니다. WGAN-GP에서 그래디언트 페널티를 사용하여 기울기의 크기를 일정하게 유지하려는 목적에 부합합니다.

##### 평가
###### 1. **Loss 그래프**:

- 생성자와 판별자 손실 모두 전반적으로 감소하는 추세를 보이는 것은 모델이 잘 학습되고 있다는 신호입니다.
- 판별자 손실의 변동은 생성자가 점차 더 현실적인 데이터를 생성함에 따라 판별자가 적응하고 있다는 것을 나타낼 수 있습니다.
###### 2. **Gradient 그래프**:

- 그래디언트 평균이 0에 가까운 값을 유지하고 있는 것은 좋은 신호입니다. 이는 기울기 소실 문제를 피하고 있음을 나타냅니다.
- 그래디언트의 변동은 모델 학습 중 발생할 수 있는 정상적인 현상입니다.

#### 3. 결론

- WGAN-GP 모델이 기울기 소실 문제를 효과적으로 해결하고 있으며, 생성자와 판별자가 균형 있게 학습되고 있음을 나타냅니다.
- 기울기 소실 문제를 해결하기 위한 Gradient Penalty가 잘 작동하고 있음을 그래디언트 평균의 변동을 통해 확인할 수 있습니다.
- BCE 손실을 사용하는 GAN은 기울기 소실 문제를 겪고 있지 않을 수 있지만, 학습 과정에서 안정적으로 최적화되지 않고 있음을 알 수 있습니다.
- WGAN-GP와 비교했을 때, BCE 손실을 사용하는 GAN은 생성자와 판별자의 손실이 안정적으로 감소하지 않으며, 그래디언트도 안정적이지 않습니다.
- 이는 WGAN-GP가 기울기 소실 문제와 모델 학습의 안정성 측면에서 더 우수하다는 것을 보여줍니다.


## 세준:
### 목표: 


### 결과:


## 우석:
### 목표:



### 결과 :



​



### 결과:


## 채연:
### 목표: 




### 결과:





### 인증샷: