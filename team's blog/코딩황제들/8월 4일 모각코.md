# 계획
프로젝트의 목표에 맞는 데이터 특성에 가장 적합한 손실 함수를 찾기 위해 다양한 손실함수에 대해 공부한다. 각 로스함수의 수학적 정의 및 유도과정 이해, 각 로스함수가 사용되는 모델과 그 이유(어떤 모델과 문제에 적합한지), 장 단점 분석, 간단한 구현예제를 작성해보면서 이해해본다. 또한 다양한 학습방법 및 디퓨전모델에 대해서도 알아본다. 더불어 energy-based model에대해서 알아본다.

**회의방법**
온라인(naver whale on) *zoom은 40분이상하려면 유료로 결제를 해야하기 때문에 whale on을 활용했습니다. 

**팀원 블로그**
박세준 https://kepler-dev-3141.github.io/
신우석 https://blog.naver.com/sws040201/
김채연 https://kcyeon0127.github.io/


# 결과

## 준혁:
### 목표: 
Wasserstein Loss를 사용한 GAN 예제 코드와 Binary Cross-Entropy (BCE) Loss를 사용한 GAN 예제코드를 작성해보면서 BCE-loss의 기울기 소실 문제와 모드 붕괴 문제의 해결책이 될 수 있는지 직접 알아본다.

### 결과:
[Compare GAN with BCE to with Wasser](file:///C:/Users/home/Documents/GitHub/mogacko/docs/lib/notebooks/compare%20to%20GAN%20with%20BCE%20to%20with%20WSTN/GAN%20TEST%20WITH%20THE%20DIFF%20LOSS.ipynb)






간단한 1차원데이터(숫자)를 랜덤으로 생성을 하여 진행하였습니다.

#### 1. GAN WITH BCE LOSS

![[team-blog-코딩황제들-2024-week4.BCELOSSlossgraph.png]]
![[team-blog-코딩황제들-2024-week4.BCELOSSgradientgraph.png]]
##### BCE Losses 그래프

- Discriminator Loss: 판별자의 손실은 초기에는 약간의 변동을 보이다가 이후 안정화되는 경향을 보입니다. 하지만 전반적인 감소 추세가 뚜렷하지 않고, 일정한 범위 내에서 변동이 반복됩니다.

- Generator Loss: 생성자의 손실은 초기에는 감소하는 경향을 보이다가 이후 증가하는 경향을 보입니다. 이는 생성자가 학습이 진행될수록 판별자를 속이는 능력이 감소하고 있음을 나타낼 수 있습니다.

##### Gradients over Steps 그래프

- **Gradient Mean**: 그래디언트 평균이 초기에는 음수에서 시작해 점차 증가하는 경향을 보입니다. 이는 기울기 소실 문제는 발생하지 않지만, 그래디언트가 매우 작거나 큰 값을 가질 수 있다는 것을 나타냅니다.


##### 평가

###### 1. **Loss 그래프**:
- 판별자와 생성자의 손실이 모두 안정적으로 감소하지 않는 점이 주목됩니다. 특히 생성자의 손실이 후반부에 증가하는 경향은 기울기 소실 문제로 인해 생성자의 학습이 제대로 이루어지지 않고 있음을 나타낼 수 있습니다.
- 손실이 감소하지 않고 변동이 계속되는 것은 모델이 최적화 과정에서 어려움을 겪고 있음을 의미할 수 있습니다.
###### 2. **Gradient 그래프**:
- 그래디언트의 평균이 증가하는 경향을 보이지만, 초기에는 매우 작은 값을 가지다가 점차 커지는 패턴을 보입니다. 이는 기울기 소실 문제는 발생하지 않지만, 그래디언트가 안정적이지 않다는 것을 나타냅니다.
- 그래디언트의 큰 변동은 모델 학습 과정에서 불안정성을 초래할 수 있습니다.

#### 2. GAN WITH WASSERSTAIN LOSS

![[team-blog-코딩황제들-2024-week4.WASSERSTAINLOSSlossgraph.png]]
![[team-blog-코딩황제들-2024-week4.WASSERSTAINLOSSgradientgraph.png]]
** GP란?
*GP는 Gradient Penalty로 모델 학습 중 기울기의 크기를 일정하게 유지하도록 강제하는 방법입니다. 이를 통해 기울기 소실 문제를 완화하고, 생성자와 판별자가 더 안정적으로 학습할 수 있게 합니다. Gradient Penalty는 실제 데이터와 생성된 데이터 사이의 보간(interpolation) 데이터를 사용하여 계산됩니다. 보간 데이터에서 기울기를 계산하고, 이 기울기가 1에 가깝도록 제약을 가합니다.*
##### WGAN-GP Losses 그래프

- **Discriminator Loss**: 전반적으로 감소하는 추세를 보이지만, 중간중간 변동이 있습니다. 이는 훈련 중 판별자가 생성자의 학습에 따라 변동하는 정상적인 현상일 수 있습니다.
- **Generator Loss**: 초기에는 거의 변동이 없지만, 점차 감소하는 추세를 보입니다. 이는 생성자가 판별자를 속이는 능력을 점차 향상시키고 있다는 것을 나타냅니다.

##### Gradients over Steps 그래프

- **Gradient Mean**: 그래디언트 평균이 0에 가까운 값을 유지하다가 중간에 급격히 증가했다가 다시 감소하는 패턴을 보입니다. 이는 기울기 소실 문제를 해결했음을 나타낼 수 있습니다. WGAN-GP에서 그래디언트 페널티를 사용하여 기울기의 크기를 일정하게 유지하려는 목적에 부합합니다.

##### 평가
###### 1. **Loss 그래프**:

- 생성자와 판별자 손실 모두 전반적으로 감소하는 추세를 보이는 것은 모델이 잘 학습되고 있다는 신호입니다.
- 판별자 손실의 변동은 생성자가 점차 더 현실적인 데이터를 생성함에 따라 판별자가 적응하고 있다는 것을 나타낼 수 있습니다.
###### 2. **Gradient 그래프**:

- 그래디언트 평균이 0에 가까운 값을 유지하고 있는 것은 좋은 신호입니다. 이는 기울기 소실 문제를 피하고 있음을 나타냅니다.
- 그래디언트의 변동은 모델 학습 중 발생할 수 있는 정상적인 현상입니다.

#### 3. 결론

- WGAN-GP 모델이 기울기 소실 문제를 효과적으로 해결하고 있으며, 생성자와 판별자가 균형 있게 학습되고 있음을 나타냅니다.
- 기울기 소실 문제를 해결하기 위한 Gradient Penalty가 잘 작동하고 있음을 그래디언트 평균의 변동을 통해 확인할 수 있습니다.
- BCE 손실을 사용하는 GAN은 기울기 소실 문제를 겪고 있지 않을 수 있지만, 학습 과정에서 안정적으로 최적화되지 않고 있음을 알 수 있습니다.
- WGAN-GP와 비교했을 때, BCE 손실을 사용하는 GAN은 생성자와 판별자의 손실이 안정적으로 감소하지 않으며, 그래디언트도 안정적이지 않습니다.
- 이는 WGAN-GP가 기울기 소실 문제와 모델 학습의 안정성 측면에서 더 우수하다는 것을 보여줍니다.



## 세준:

### 목표:

EBGAN(Energy-Based GAN)의 개념 알기

### 결과:

#### Introduction

기존 GAN은 Generator는 가짜 데이터를 생성하고 Discriminator는 Generator가 생성한 데이터와 진짜 데이터를 구별해가며 Discriminator가 진짜와 가짜를 구별할 수 없을 정도로 Generator잘 학습시키는 것이다.

“Pasted image 20240805222239.png” could not be found.

> **Energy-model + Generative Adversarial Network = Energy-based Adversarial Network**

기존 GAN과 Energy-based model을 결합한 것이 EBGAN이고, GAN과 Auto-Encoder를 결합한 구조이다.

“Pasted image 20240805224001.png” could not be found.

#### Loss Function

Discriminator의 Loss는 진짜 데이터 를 Discriminator에 통과시킨 값과 hinge loss로 정의된 를 더한 값이다. 양수 보다 가 작은 경우에 대해서만 hinge loss가 양수가 됩니다.  
Generator의 Loss는 Generator가 노이즈 로부터 생성한 가짜 데이터를 Discriminator에 통과시켜 나온 값이다.

“Pasted image 20240805233728.png” could not be found.


## 우석:
### 목표:

softmax에 대해 알아본다.

### 결과 :

sigmoid를 다중 클래스 분류로 일반화하여 softmax를 유도해봤다.

ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ

softmax는 sigmoid 함수를 다중 클래스 분류로 일반화하면 유도할 수 있다.

$odds=\frac{P\left(C_1 \mid X\right)}{P\left({C}_2\mid X\right)}={e}^z$

클래스가 k개 일때로 일반화 하면

$\frac{P\left({C}_i\mid X\right)}{P\left({C}_k\mid X\right)}={e}^{{z}_i}$

양변을 i=1부터 k-1까지 더하면

$\sum _{i=1}^{k-1}\frac{P\left({C}_i\mid X\right)}{P\left({C}_k\mid X\right)}=\sum _{i=1}^{k-1}{e}^{{z}_i}$

$\frac{P\left({C}_1\mid X\right)+P\left({C}_2\mid X\right)+\cdot \cdot \cdot +P\left({C}_{k-1}\mid X\right)}{P\left({C}_k\mid X\right)}=\sum _{i=1}^{k-1}{e}^{{z}_i}$

$\frac{1-P\left({C}_k\mid X\right)}{P\left({C}_k\mid X\right)}=\sum _{i=1}^{k-1}{e}^{{z}_i}$

$P\left({C}_k\mid X\right)=\frac{1}{1+\sum _{i=1}^{k-1}{e}^{{z}_i}}$​​​

odds를 일반화 한 식에서 P(Ci|X)에 대해 정리하면

$P\left({C}_i\mid X\right)={e}^{{z}_i}P\left({C}_k\mid X\right)$

$P\left({C}_i\mid X\right)=\frac{{e}^{{z}_i}}{1+\sum _{i=1}^{k-1}{e}^{{z}_i}}$


i 대신 k를 넣으면 1이므로

$\frac{P\left({C}_k\mid X\right)}{P\left({C}_k\mid X\right)}={e}^{{z}_k}=1$


이를 이용하면 softmax가 된다.

**$P\left({C}_i\mid X\right)=\frac{{e}^{{z}_i}}{\sum _{i=1}^k{e}^{{z}_i}}$

​

**[출처]** [sigmoid, softmax (8/05)](https://blog.naver.com/sws040201/223537751754)| **작성자** [sws040201](https://blog.naver.com/sws040201)


## 채연:

### 목표:
Energy-Based Model (EBM)에 대해 알아본다.

Energy-Based Model (EBM) Energy-Based Model은 데이터 x의 에너지를 계산하여 확률 분포를 모델링하는 방식의 생성 모델이다. 확률을 에너지 함수로 표현하고, 이 에너지를 최소화하는 데이터를 더 높은 확률로 간주하는 것.

데이터의 에너지 (Energy of Data) 에너지 기반 모델(Energy-Based Model, EBM)에서 “에너지”는 데이터 x가 특정 상태에 있을 때의 “비용” 또는 “불일치”를 나타내는 값이다. 에너지가 낮을수록 해당 데이터가 모델이 예상하는 더 가능성 있는 상태를 나타냅니다. 즉, 에너지가 낮은 데이터는 모델에 의해 더 높은 확률로 간주됩니다. 에너지 함수 (Energy Function) 에너지 함수 E(x)는 주어진 데이터 x의 에너지를 계산하는 함수입니다. 이 함수는 데이터 x와 모델 파라미터 θ를 입력으로 받아 에너지 값을 출력합니다. 에너지 함수는 모델의 학습 과정에서 중요한 역할을 하며, 데이터의 패턴이나 구조를 반영하도록 설계됩니다.

Energy-Based Models (EBMs)의 학습 목적은 주어진 데이터 분포를 잘 모델링하고, 이를 통해 새로운 데이터를 생성하거나, 주어진 데이터의 패턴과 구조를 이해하는 것입니다. 에너지 함수 정의:

데이터 𝑥 에 대한 에너지 함수 𝐸(𝑥: 𝜃) 를 정의 이 함수는 모델의 파라미터 𝜃를 포함, 데이터의 패턴과 구조를 반영

데이터는 모델이 학습할 분포를 대표할 수 있어야한다.

에너지 함수 최적화: 에너지 함수의 파라미터 θ를 최적화하여, 낮은 에너지를 가지는 데이터가 높은 확률을 갖도록 이 과정에서 손실 함수(Loss Function)를 정의하고, 이를 최소화하는 방향으로 파라미터를 업데이트

정규화 상수 근사: 정규화 상수 (Z)는 직접 계산하기 어려운 경우가 많아, Markov Chain Monte Carlo (MCMC)와 같은 샘플링 기법을 사용하여 근사 모델의 확률 분포를 정규화






### 인증샷:
![[team-blog-코딩황제들-2024-week4.모각코4일차회의인증.png]]
![[team-blog-코딩황제들-2024-week4.모각코4일차시간인증.jpg]]